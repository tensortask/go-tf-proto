// Code generated by protoc-gen-gogo. DO NOT EDIT.
// source: step_stats.proto

package framework

import proto "github.com/gogo/protobuf/proto"
import fmt "fmt"
import math "math"
import _ "github.com/gogo/protobuf/gogoproto"

import strings "strings"
import reflect "reflect"

import io "io"

// Reference imports to suppress errors if they are not otherwise used.
var _ = proto.Marshal
var _ = fmt.Errorf
var _ = math.Inf

// This is a compile-time assertion to ensure that this generated file
// is compatible with the proto package it is being compiled against.
// A compilation error at this line likely means your copy of the
// proto package needs to be updated.
const _ = proto.GoGoProtoPackageIsVersion2 // please upgrade the proto package

// An allocation/de-allocation operation performed by the allocator.
type AllocationRecord struct {
	// The timestamp of the operation.
	AllocMicros int64 `protobuf:"varint,1,opt,name=alloc_micros,json=allocMicros,proto3" json:"alloc_micros,omitempty"`
	// Number of bytes allocated, or de-allocated if negative.
	AllocBytes           int64    `protobuf:"varint,2,opt,name=alloc_bytes,json=allocBytes,proto3" json:"alloc_bytes,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *AllocationRecord) Reset()      { *m = AllocationRecord{} }
func (*AllocationRecord) ProtoMessage() {}
func (*AllocationRecord) Descriptor() ([]byte, []int) {
	return fileDescriptor_step_stats_fae30a98b1fe86d1, []int{0}
}
func (m *AllocationRecord) XXX_Unmarshal(b []byte) error {
	return m.Unmarshal(b)
}
func (m *AllocationRecord) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	if deterministic {
		return xxx_messageInfo_AllocationRecord.Marshal(b, m, deterministic)
	} else {
		b = b[:cap(b)]
		n, err := m.MarshalTo(b)
		if err != nil {
			return nil, err
		}
		return b[:n], nil
	}
}
func (dst *AllocationRecord) XXX_Merge(src proto.Message) {
	xxx_messageInfo_AllocationRecord.Merge(dst, src)
}
func (m *AllocationRecord) XXX_Size() int {
	return m.Size()
}
func (m *AllocationRecord) XXX_DiscardUnknown() {
	xxx_messageInfo_AllocationRecord.DiscardUnknown(m)
}

var xxx_messageInfo_AllocationRecord proto.InternalMessageInfo

func (m *AllocationRecord) GetAllocMicros() int64 {
	if m != nil {
		return m.AllocMicros
	}
	return 0
}

func (m *AllocationRecord) GetAllocBytes() int64 {
	if m != nil {
		return m.AllocBytes
	}
	return 0
}

type AllocatorMemoryUsed struct {
	AllocatorName string `protobuf:"bytes,1,opt,name=allocator_name,json=allocatorName,proto3" json:"allocator_name,omitempty"`
	// These are per-node allocator memory stats.
	TotalBytes int64 `protobuf:"varint,2,opt,name=total_bytes,json=totalBytes,proto3" json:"total_bytes,omitempty"`
	PeakBytes  int64 `protobuf:"varint,3,opt,name=peak_bytes,json=peakBytes,proto3" json:"peak_bytes,omitempty"`
	// The bytes that are not deallocated.
	LiveBytes int64 `protobuf:"varint,4,opt,name=live_bytes,json=liveBytes,proto3" json:"live_bytes,omitempty"`
	// The allocation and deallocation timeline.
	AllocationRecords []*AllocationRecord `protobuf:"bytes,6,rep,name=allocation_records,json=allocationRecords" json:"allocation_records,omitempty"`
	// These are snapshots of the overall allocator memory stats.
	// The number of live bytes currently allocated by the allocator.
	AllocatorBytesInUse  int64    `protobuf:"varint,5,opt,name=allocator_bytes_in_use,json=allocatorBytesInUse,proto3" json:"allocator_bytes_in_use,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *AllocatorMemoryUsed) Reset()      { *m = AllocatorMemoryUsed{} }
func (*AllocatorMemoryUsed) ProtoMessage() {}
func (*AllocatorMemoryUsed) Descriptor() ([]byte, []int) {
	return fileDescriptor_step_stats_fae30a98b1fe86d1, []int{1}
}
func (m *AllocatorMemoryUsed) XXX_Unmarshal(b []byte) error {
	return m.Unmarshal(b)
}
func (m *AllocatorMemoryUsed) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	if deterministic {
		return xxx_messageInfo_AllocatorMemoryUsed.Marshal(b, m, deterministic)
	} else {
		b = b[:cap(b)]
		n, err := m.MarshalTo(b)
		if err != nil {
			return nil, err
		}
		return b[:n], nil
	}
}
func (dst *AllocatorMemoryUsed) XXX_Merge(src proto.Message) {
	xxx_messageInfo_AllocatorMemoryUsed.Merge(dst, src)
}
func (m *AllocatorMemoryUsed) XXX_Size() int {
	return m.Size()
}
func (m *AllocatorMemoryUsed) XXX_DiscardUnknown() {
	xxx_messageInfo_AllocatorMemoryUsed.DiscardUnknown(m)
}

var xxx_messageInfo_AllocatorMemoryUsed proto.InternalMessageInfo

func (m *AllocatorMemoryUsed) GetAllocatorName() string {
	if m != nil {
		return m.AllocatorName
	}
	return ""
}

func (m *AllocatorMemoryUsed) GetTotalBytes() int64 {
	if m != nil {
		return m.TotalBytes
	}
	return 0
}

func (m *AllocatorMemoryUsed) GetPeakBytes() int64 {
	if m != nil {
		return m.PeakBytes
	}
	return 0
}

func (m *AllocatorMemoryUsed) GetLiveBytes() int64 {
	if m != nil {
		return m.LiveBytes
	}
	return 0
}

func (m *AllocatorMemoryUsed) GetAllocationRecords() []*AllocationRecord {
	if m != nil {
		return m.AllocationRecords
	}
	return nil
}

func (m *AllocatorMemoryUsed) GetAllocatorBytesInUse() int64 {
	if m != nil {
		return m.AllocatorBytesInUse
	}
	return 0
}

// Output sizes recorded for a single execution of a graph node.
type NodeOutput struct {
	Slot                 int32              `protobuf:"varint,1,opt,name=slot,proto3" json:"slot,omitempty"`
	TensorDescription    *TensorDescription `protobuf:"bytes,3,opt,name=tensor_description,json=tensorDescription" json:"tensor_description,omitempty"`
	XXX_NoUnkeyedLiteral struct{}           `json:"-"`
	XXX_sizecache        int32              `json:"-"`
}

func (m *NodeOutput) Reset()      { *m = NodeOutput{} }
func (*NodeOutput) ProtoMessage() {}
func (*NodeOutput) Descriptor() ([]byte, []int) {
	return fileDescriptor_step_stats_fae30a98b1fe86d1, []int{2}
}
func (m *NodeOutput) XXX_Unmarshal(b []byte) error {
	return m.Unmarshal(b)
}
func (m *NodeOutput) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	if deterministic {
		return xxx_messageInfo_NodeOutput.Marshal(b, m, deterministic)
	} else {
		b = b[:cap(b)]
		n, err := m.MarshalTo(b)
		if err != nil {
			return nil, err
		}
		return b[:n], nil
	}
}
func (dst *NodeOutput) XXX_Merge(src proto.Message) {
	xxx_messageInfo_NodeOutput.Merge(dst, src)
}
func (m *NodeOutput) XXX_Size() int {
	return m.Size()
}
func (m *NodeOutput) XXX_DiscardUnknown() {
	xxx_messageInfo_NodeOutput.DiscardUnknown(m)
}

var xxx_messageInfo_NodeOutput proto.InternalMessageInfo

func (m *NodeOutput) GetSlot() int32 {
	if m != nil {
		return m.Slot
	}
	return 0
}

func (m *NodeOutput) GetTensorDescription() *TensorDescription {
	if m != nil {
		return m.TensorDescription
	}
	return nil
}

// For memory tracking.
type MemoryStats struct {
	TempMemorySize                 int64    `protobuf:"varint,1,opt,name=temp_memory_size,json=tempMemorySize,proto3" json:"temp_memory_size,omitempty"`
	PersistentMemorySize           int64    `protobuf:"varint,3,opt,name=persistent_memory_size,json=persistentMemorySize,proto3" json:"persistent_memory_size,omitempty"`
	PersistentTensorAllocIds       []int64  `protobuf:"varint,5,rep,packed,name=persistent_tensor_alloc_ids,json=persistentTensorAllocIds" json:"persistent_tensor_alloc_ids,omitempty"`
	DeviceTempMemorySize           int64    `protobuf:"varint,2,opt,name=device_temp_memory_size,json=deviceTempMemorySize,proto3" json:"device_temp_memory_size,omitempty"`                                 // Deprecated: Do not use.
	DevicePersistentMemorySize     int64    `protobuf:"varint,4,opt,name=device_persistent_memory_size,json=devicePersistentMemorySize,proto3" json:"device_persistent_memory_size,omitempty"`               // Deprecated: Do not use.
	DevicePersistentTensorAllocIds []int64  `protobuf:"varint,6,rep,packed,name=device_persistent_tensor_alloc_ids,json=devicePersistentTensorAllocIds" json:"device_persistent_tensor_alloc_ids,omitempty"` // Deprecated: Do not use.
	XXX_NoUnkeyedLiteral           struct{} `json:"-"`
	XXX_sizecache                  int32    `json:"-"`
}

func (m *MemoryStats) Reset()      { *m = MemoryStats{} }
func (*MemoryStats) ProtoMessage() {}
func (*MemoryStats) Descriptor() ([]byte, []int) {
	return fileDescriptor_step_stats_fae30a98b1fe86d1, []int{3}
}
func (m *MemoryStats) XXX_Unmarshal(b []byte) error {
	return m.Unmarshal(b)
}
func (m *MemoryStats) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	if deterministic {
		return xxx_messageInfo_MemoryStats.Marshal(b, m, deterministic)
	} else {
		b = b[:cap(b)]
		n, err := m.MarshalTo(b)
		if err != nil {
			return nil, err
		}
		return b[:n], nil
	}
}
func (dst *MemoryStats) XXX_Merge(src proto.Message) {
	xxx_messageInfo_MemoryStats.Merge(dst, src)
}
func (m *MemoryStats) XXX_Size() int {
	return m.Size()
}
func (m *MemoryStats) XXX_DiscardUnknown() {
	xxx_messageInfo_MemoryStats.DiscardUnknown(m)
}

var xxx_messageInfo_MemoryStats proto.InternalMessageInfo

func (m *MemoryStats) GetTempMemorySize() int64 {
	if m != nil {
		return m.TempMemorySize
	}
	return 0
}

func (m *MemoryStats) GetPersistentMemorySize() int64 {
	if m != nil {
		return m.PersistentMemorySize
	}
	return 0
}

func (m *MemoryStats) GetPersistentTensorAllocIds() []int64 {
	if m != nil {
		return m.PersistentTensorAllocIds
	}
	return nil
}

// Deprecated: Do not use.
func (m *MemoryStats) GetDeviceTempMemorySize() int64 {
	if m != nil {
		return m.DeviceTempMemorySize
	}
	return 0
}

// Deprecated: Do not use.
func (m *MemoryStats) GetDevicePersistentMemorySize() int64 {
	if m != nil {
		return m.DevicePersistentMemorySize
	}
	return 0
}

// Deprecated: Do not use.
func (m *MemoryStats) GetDevicePersistentTensorAllocIds() []int64 {
	if m != nil {
		return m.DevicePersistentTensorAllocIds
	}
	return nil
}

// Time/size stats recorded for a single execution of a graph node.
type NodeExecStats struct {
	// TODO(tucker): Use some more compact form of node identity than
	// the full string name.  Either all processes should agree on a
	// global id (cost_id?) for each node, or we should use a hash of
	// the name.
	NodeName             string                   `protobuf:"bytes,1,opt,name=node_name,json=nodeName,proto3" json:"node_name,omitempty"`
	AllStartMicros       int64                    `protobuf:"varint,2,opt,name=all_start_micros,json=allStartMicros,proto3" json:"all_start_micros,omitempty"`
	OpStartRelMicros     int64                    `protobuf:"varint,3,opt,name=op_start_rel_micros,json=opStartRelMicros,proto3" json:"op_start_rel_micros,omitempty"`
	OpEndRelMicros       int64                    `protobuf:"varint,4,opt,name=op_end_rel_micros,json=opEndRelMicros,proto3" json:"op_end_rel_micros,omitempty"`
	AllEndRelMicros      int64                    `protobuf:"varint,5,opt,name=all_end_rel_micros,json=allEndRelMicros,proto3" json:"all_end_rel_micros,omitempty"`
	Memory               []*AllocatorMemoryUsed   `protobuf:"bytes,6,rep,name=memory" json:"memory,omitempty"`
	Output               []*NodeOutput            `protobuf:"bytes,7,rep,name=output" json:"output,omitempty"`
	TimelineLabel        string                   `protobuf:"bytes,8,opt,name=timeline_label,json=timelineLabel,proto3" json:"timeline_label,omitempty"`
	ScheduledMicros      int64                    `protobuf:"varint,9,opt,name=scheduled_micros,json=scheduledMicros,proto3" json:"scheduled_micros,omitempty"`
	ThreadId             uint32                   `protobuf:"varint,10,opt,name=thread_id,json=threadId,proto3" json:"thread_id,omitempty"`
	ReferencedTensor     []*AllocationDescription `protobuf:"bytes,11,rep,name=referenced_tensor,json=referencedTensor" json:"referenced_tensor,omitempty"`
	MemoryStats          *MemoryStats             `protobuf:"bytes,12,opt,name=memory_stats,json=memoryStats" json:"memory_stats,omitempty"`
	AllStartNanos        int64                    `protobuf:"varint,13,opt,name=all_start_nanos,json=allStartNanos,proto3" json:"all_start_nanos,omitempty"`
	OpStartRelNanos      int64                    `protobuf:"varint,14,opt,name=op_start_rel_nanos,json=opStartRelNanos,proto3" json:"op_start_rel_nanos,omitempty"`
	OpEndRelNanos        int64                    `protobuf:"varint,15,opt,name=op_end_rel_nanos,json=opEndRelNanos,proto3" json:"op_end_rel_nanos,omitempty"`
	AllEndRelNanos       int64                    `protobuf:"varint,16,opt,name=all_end_rel_nanos,json=allEndRelNanos,proto3" json:"all_end_rel_nanos,omitempty"`
	ScheduledNanos       int64                    `protobuf:"varint,17,opt,name=scheduled_nanos,json=scheduledNanos,proto3" json:"scheduled_nanos,omitempty"`
	XXX_NoUnkeyedLiteral struct{}                 `json:"-"`
	XXX_sizecache        int32                    `json:"-"`
}

func (m *NodeExecStats) Reset()      { *m = NodeExecStats{} }
func (*NodeExecStats) ProtoMessage() {}
func (*NodeExecStats) Descriptor() ([]byte, []int) {
	return fileDescriptor_step_stats_fae30a98b1fe86d1, []int{4}
}
func (m *NodeExecStats) XXX_Unmarshal(b []byte) error {
	return m.Unmarshal(b)
}
func (m *NodeExecStats) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	if deterministic {
		return xxx_messageInfo_NodeExecStats.Marshal(b, m, deterministic)
	} else {
		b = b[:cap(b)]
		n, err := m.MarshalTo(b)
		if err != nil {
			return nil, err
		}
		return b[:n], nil
	}
}
func (dst *NodeExecStats) XXX_Merge(src proto.Message) {
	xxx_messageInfo_NodeExecStats.Merge(dst, src)
}
func (m *NodeExecStats) XXX_Size() int {
	return m.Size()
}
func (m *NodeExecStats) XXX_DiscardUnknown() {
	xxx_messageInfo_NodeExecStats.DiscardUnknown(m)
}

var xxx_messageInfo_NodeExecStats proto.InternalMessageInfo

func (m *NodeExecStats) GetNodeName() string {
	if m != nil {
		return m.NodeName
	}
	return ""
}

func (m *NodeExecStats) GetAllStartMicros() int64 {
	if m != nil {
		return m.AllStartMicros
	}
	return 0
}

func (m *NodeExecStats) GetOpStartRelMicros() int64 {
	if m != nil {
		return m.OpStartRelMicros
	}
	return 0
}

func (m *NodeExecStats) GetOpEndRelMicros() int64 {
	if m != nil {
		return m.OpEndRelMicros
	}
	return 0
}

func (m *NodeExecStats) GetAllEndRelMicros() int64 {
	if m != nil {
		return m.AllEndRelMicros
	}
	return 0
}

func (m *NodeExecStats) GetMemory() []*AllocatorMemoryUsed {
	if m != nil {
		return m.Memory
	}
	return nil
}

func (m *NodeExecStats) GetOutput() []*NodeOutput {
	if m != nil {
		return m.Output
	}
	return nil
}

func (m *NodeExecStats) GetTimelineLabel() string {
	if m != nil {
		return m.TimelineLabel
	}
	return ""
}

func (m *NodeExecStats) GetScheduledMicros() int64 {
	if m != nil {
		return m.ScheduledMicros
	}
	return 0
}

func (m *NodeExecStats) GetThreadId() uint32 {
	if m != nil {
		return m.ThreadId
	}
	return 0
}

func (m *NodeExecStats) GetReferencedTensor() []*AllocationDescription {
	if m != nil {
		return m.ReferencedTensor
	}
	return nil
}

func (m *NodeExecStats) GetMemoryStats() *MemoryStats {
	if m != nil {
		return m.MemoryStats
	}
	return nil
}

func (m *NodeExecStats) GetAllStartNanos() int64 {
	if m != nil {
		return m.AllStartNanos
	}
	return 0
}

func (m *NodeExecStats) GetOpStartRelNanos() int64 {
	if m != nil {
		return m.OpStartRelNanos
	}
	return 0
}

func (m *NodeExecStats) GetOpEndRelNanos() int64 {
	if m != nil {
		return m.OpEndRelNanos
	}
	return 0
}

func (m *NodeExecStats) GetAllEndRelNanos() int64 {
	if m != nil {
		return m.AllEndRelNanos
	}
	return 0
}

func (m *NodeExecStats) GetScheduledNanos() int64 {
	if m != nil {
		return m.ScheduledNanos
	}
	return 0
}

type DeviceStepStats struct {
	Device               string           `protobuf:"bytes,1,opt,name=device,proto3" json:"device,omitempty"`
	NodeStats            []*NodeExecStats `protobuf:"bytes,2,rep,name=node_stats,json=nodeStats" json:"node_stats,omitempty"`
	XXX_NoUnkeyedLiteral struct{}         `json:"-"`
	XXX_sizecache        int32            `json:"-"`
}

func (m *DeviceStepStats) Reset()      { *m = DeviceStepStats{} }
func (*DeviceStepStats) ProtoMessage() {}
func (*DeviceStepStats) Descriptor() ([]byte, []int) {
	return fileDescriptor_step_stats_fae30a98b1fe86d1, []int{5}
}
func (m *DeviceStepStats) XXX_Unmarshal(b []byte) error {
	return m.Unmarshal(b)
}
func (m *DeviceStepStats) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	if deterministic {
		return xxx_messageInfo_DeviceStepStats.Marshal(b, m, deterministic)
	} else {
		b = b[:cap(b)]
		n, err := m.MarshalTo(b)
		if err != nil {
			return nil, err
		}
		return b[:n], nil
	}
}
func (dst *DeviceStepStats) XXX_Merge(src proto.Message) {
	xxx_messageInfo_DeviceStepStats.Merge(dst, src)
}
func (m *DeviceStepStats) XXX_Size() int {
	return m.Size()
}
func (m *DeviceStepStats) XXX_DiscardUnknown() {
	xxx_messageInfo_DeviceStepStats.DiscardUnknown(m)
}

var xxx_messageInfo_DeviceStepStats proto.InternalMessageInfo

func (m *DeviceStepStats) GetDevice() string {
	if m != nil {
		return m.Device
	}
	return ""
}

func (m *DeviceStepStats) GetNodeStats() []*NodeExecStats {
	if m != nil {
		return m.NodeStats
	}
	return nil
}

type StepStats struct {
	DevStats             []*DeviceStepStats `protobuf:"bytes,1,rep,name=dev_stats,json=devStats" json:"dev_stats,omitempty"`
	XXX_NoUnkeyedLiteral struct{}           `json:"-"`
	XXX_sizecache        int32              `json:"-"`
}

func (m *StepStats) Reset()      { *m = StepStats{} }
func (*StepStats) ProtoMessage() {}
func (*StepStats) Descriptor() ([]byte, []int) {
	return fileDescriptor_step_stats_fae30a98b1fe86d1, []int{6}
}
func (m *StepStats) XXX_Unmarshal(b []byte) error {
	return m.Unmarshal(b)
}
func (m *StepStats) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	if deterministic {
		return xxx_messageInfo_StepStats.Marshal(b, m, deterministic)
	} else {
		b = b[:cap(b)]
		n, err := m.MarshalTo(b)
		if err != nil {
			return nil, err
		}
		return b[:n], nil
	}
}
func (dst *StepStats) XXX_Merge(src proto.Message) {
	xxx_messageInfo_StepStats.Merge(dst, src)
}
func (m *StepStats) XXX_Size() int {
	return m.Size()
}
func (m *StepStats) XXX_DiscardUnknown() {
	xxx_messageInfo_StepStats.DiscardUnknown(m)
}

var xxx_messageInfo_StepStats proto.InternalMessageInfo

func (m *StepStats) GetDevStats() []*DeviceStepStats {
	if m != nil {
		return m.DevStats
	}
	return nil
}

func init() {
	proto.RegisterType((*AllocationRecord)(nil), "framework.AllocationRecord")
	proto.RegisterType((*AllocatorMemoryUsed)(nil), "framework.AllocatorMemoryUsed")
	proto.RegisterType((*NodeOutput)(nil), "framework.NodeOutput")
	proto.RegisterType((*MemoryStats)(nil), "framework.MemoryStats")
	proto.RegisterType((*NodeExecStats)(nil), "framework.NodeExecStats")
	proto.RegisterType((*DeviceStepStats)(nil), "framework.DeviceStepStats")
	proto.RegisterType((*StepStats)(nil), "framework.StepStats")
}
func (this *AllocationRecord) VerboseEqual(that interface{}) error {
	if that == nil {
		if this == nil {
			return nil
		}
		return fmt.Errorf("that == nil && this != nil")
	}

	that1, ok := that.(*AllocationRecord)
	if !ok {
		that2, ok := that.(AllocationRecord)
		if ok {
			that1 = &that2
		} else {
			return fmt.Errorf("that is not of type *AllocationRecord")
		}
	}
	if that1 == nil {
		if this == nil {
			return nil
		}
		return fmt.Errorf("that is type *AllocationRecord but is nil && this != nil")
	} else if this == nil {
		return fmt.Errorf("that is type *AllocationRecord but is not nil && this == nil")
	}
	if this.AllocMicros != that1.AllocMicros {
		return fmt.Errorf("AllocMicros this(%v) Not Equal that(%v)", this.AllocMicros, that1.AllocMicros)
	}
	if this.AllocBytes != that1.AllocBytes {
		return fmt.Errorf("AllocBytes this(%v) Not Equal that(%v)", this.AllocBytes, that1.AllocBytes)
	}
	return nil
}
func (this *AllocationRecord) Equal(that interface{}) bool {
	if that == nil {
		return this == nil
	}

	that1, ok := that.(*AllocationRecord)
	if !ok {
		that2, ok := that.(AllocationRecord)
		if ok {
			that1 = &that2
		} else {
			return false
		}
	}
	if that1 == nil {
		return this == nil
	} else if this == nil {
		return false
	}
	if this.AllocMicros != that1.AllocMicros {
		return false
	}
	if this.AllocBytes != that1.AllocBytes {
		return false
	}
	return true
}
func (this *AllocatorMemoryUsed) VerboseEqual(that interface{}) error {
	if that == nil {
		if this == nil {
			return nil
		}
		return fmt.Errorf("that == nil && this != nil")
	}

	that1, ok := that.(*AllocatorMemoryUsed)
	if !ok {
		that2, ok := that.(AllocatorMemoryUsed)
		if ok {
			that1 = &that2
		} else {
			return fmt.Errorf("that is not of type *AllocatorMemoryUsed")
		}
	}
	if that1 == nil {
		if this == nil {
			return nil
		}
		return fmt.Errorf("that is type *AllocatorMemoryUsed but is nil && this != nil")
	} else if this == nil {
		return fmt.Errorf("that is type *AllocatorMemoryUsed but is not nil && this == nil")
	}
	if this.AllocatorName != that1.AllocatorName {
		return fmt.Errorf("AllocatorName this(%v) Not Equal that(%v)", this.AllocatorName, that1.AllocatorName)
	}
	if this.TotalBytes != that1.TotalBytes {
		return fmt.Errorf("TotalBytes this(%v) Not Equal that(%v)", this.TotalBytes, that1.TotalBytes)
	}
	if this.PeakBytes != that1.PeakBytes {
		return fmt.Errorf("PeakBytes this(%v) Not Equal that(%v)", this.PeakBytes, that1.PeakBytes)
	}
	if this.LiveBytes != that1.LiveBytes {
		return fmt.Errorf("LiveBytes this(%v) Not Equal that(%v)", this.LiveBytes, that1.LiveBytes)
	}
	if len(this.AllocationRecords) != len(that1.AllocationRecords) {
		return fmt.Errorf("AllocationRecords this(%v) Not Equal that(%v)", len(this.AllocationRecords), len(that1.AllocationRecords))
	}
	for i := range this.AllocationRecords {
		if !this.AllocationRecords[i].Equal(that1.AllocationRecords[i]) {
			return fmt.Errorf("AllocationRecords this[%v](%v) Not Equal that[%v](%v)", i, this.AllocationRecords[i], i, that1.AllocationRecords[i])
		}
	}
	if this.AllocatorBytesInUse != that1.AllocatorBytesInUse {
		return fmt.Errorf("AllocatorBytesInUse this(%v) Not Equal that(%v)", this.AllocatorBytesInUse, that1.AllocatorBytesInUse)
	}
	return nil
}
func (this *AllocatorMemoryUsed) Equal(that interface{}) bool {
	if that == nil {
		return this == nil
	}

	that1, ok := that.(*AllocatorMemoryUsed)
	if !ok {
		that2, ok := that.(AllocatorMemoryUsed)
		if ok {
			that1 = &that2
		} else {
			return false
		}
	}
	if that1 == nil {
		return this == nil
	} else if this == nil {
		return false
	}
	if this.AllocatorName != that1.AllocatorName {
		return false
	}
	if this.TotalBytes != that1.TotalBytes {
		return false
	}
	if this.PeakBytes != that1.PeakBytes {
		return false
	}
	if this.LiveBytes != that1.LiveBytes {
		return false
	}
	if len(this.AllocationRecords) != len(that1.AllocationRecords) {
		return false
	}
	for i := range this.AllocationRecords {
		if !this.AllocationRecords[i].Equal(that1.AllocationRecords[i]) {
			return false
		}
	}
	if this.AllocatorBytesInUse != that1.AllocatorBytesInUse {
		return false
	}
	return true
}
func (this *NodeOutput) VerboseEqual(that interface{}) error {
	if that == nil {
		if this == nil {
			return nil
		}
		return fmt.Errorf("that == nil && this != nil")
	}

	that1, ok := that.(*NodeOutput)
	if !ok {
		that2, ok := that.(NodeOutput)
		if ok {
			that1 = &that2
		} else {
			return fmt.Errorf("that is not of type *NodeOutput")
		}
	}
	if that1 == nil {
		if this == nil {
			return nil
		}
		return fmt.Errorf("that is type *NodeOutput but is nil && this != nil")
	} else if this == nil {
		return fmt.Errorf("that is type *NodeOutput but is not nil && this == nil")
	}
	if this.Slot != that1.Slot {
		return fmt.Errorf("Slot this(%v) Not Equal that(%v)", this.Slot, that1.Slot)
	}
	if !this.TensorDescription.Equal(that1.TensorDescription) {
		return fmt.Errorf("TensorDescription this(%v) Not Equal that(%v)", this.TensorDescription, that1.TensorDescription)
	}
	return nil
}
func (this *NodeOutput) Equal(that interface{}) bool {
	if that == nil {
		return this == nil
	}

	that1, ok := that.(*NodeOutput)
	if !ok {
		that2, ok := that.(NodeOutput)
		if ok {
			that1 = &that2
		} else {
			return false
		}
	}
	if that1 == nil {
		return this == nil
	} else if this == nil {
		return false
	}
	if this.Slot != that1.Slot {
		return false
	}
	if !this.TensorDescription.Equal(that1.TensorDescription) {
		return false
	}
	return true
}
func (this *MemoryStats) VerboseEqual(that interface{}) error {
	if that == nil {
		if this == nil {
			return nil
		}
		return fmt.Errorf("that == nil && this != nil")
	}

	that1, ok := that.(*MemoryStats)
	if !ok {
		that2, ok := that.(MemoryStats)
		if ok {
			that1 = &that2
		} else {
			return fmt.Errorf("that is not of type *MemoryStats")
		}
	}
	if that1 == nil {
		if this == nil {
			return nil
		}
		return fmt.Errorf("that is type *MemoryStats but is nil && this != nil")
	} else if this == nil {
		return fmt.Errorf("that is type *MemoryStats but is not nil && this == nil")
	}
	if this.TempMemorySize != that1.TempMemorySize {
		return fmt.Errorf("TempMemorySize this(%v) Not Equal that(%v)", this.TempMemorySize, that1.TempMemorySize)
	}
	if this.PersistentMemorySize != that1.PersistentMemorySize {
		return fmt.Errorf("PersistentMemorySize this(%v) Not Equal that(%v)", this.PersistentMemorySize, that1.PersistentMemorySize)
	}
	if len(this.PersistentTensorAllocIds) != len(that1.PersistentTensorAllocIds) {
		return fmt.Errorf("PersistentTensorAllocIds this(%v) Not Equal that(%v)", len(this.PersistentTensorAllocIds), len(that1.PersistentTensorAllocIds))
	}
	for i := range this.PersistentTensorAllocIds {
		if this.PersistentTensorAllocIds[i] != that1.PersistentTensorAllocIds[i] {
			return fmt.Errorf("PersistentTensorAllocIds this[%v](%v) Not Equal that[%v](%v)", i, this.PersistentTensorAllocIds[i], i, that1.PersistentTensorAllocIds[i])
		}
	}
	if this.DeviceTempMemorySize != that1.DeviceTempMemorySize {
		return fmt.Errorf("DeviceTempMemorySize this(%v) Not Equal that(%v)", this.DeviceTempMemorySize, that1.DeviceTempMemorySize)
	}
	if this.DevicePersistentMemorySize != that1.DevicePersistentMemorySize {
		return fmt.Errorf("DevicePersistentMemorySize this(%v) Not Equal that(%v)", this.DevicePersistentMemorySize, that1.DevicePersistentMemorySize)
	}
	if len(this.DevicePersistentTensorAllocIds) != len(that1.DevicePersistentTensorAllocIds) {
		return fmt.Errorf("DevicePersistentTensorAllocIds this(%v) Not Equal that(%v)", len(this.DevicePersistentTensorAllocIds), len(that1.DevicePersistentTensorAllocIds))
	}
	for i := range this.DevicePersistentTensorAllocIds {
		if this.DevicePersistentTensorAllocIds[i] != that1.DevicePersistentTensorAllocIds[i] {
			return fmt.Errorf("DevicePersistentTensorAllocIds this[%v](%v) Not Equal that[%v](%v)", i, this.DevicePersistentTensorAllocIds[i], i, that1.DevicePersistentTensorAllocIds[i])
		}
	}
	return nil
}
func (this *MemoryStats) Equal(that interface{}) bool {
	if that == nil {
		return this == nil
	}

	that1, ok := that.(*MemoryStats)
	if !ok {
		that2, ok := that.(MemoryStats)
		if ok {
			that1 = &that2
		} else {
			return false
		}
	}
	if that1 == nil {
		return this == nil
	} else if this == nil {
		return false
	}
	if this.TempMemorySize != that1.TempMemorySize {
		return false
	}
	if this.PersistentMemorySize != that1.PersistentMemorySize {
		return false
	}
	if len(this.PersistentTensorAllocIds) != len(that1.PersistentTensorAllocIds) {
		return false
	}
	for i := range this.PersistentTensorAllocIds {
		if this.PersistentTensorAllocIds[i] != that1.PersistentTensorAllocIds[i] {
			return false
		}
	}
	if this.DeviceTempMemorySize != that1.DeviceTempMemorySize {
		return false
	}
	if this.DevicePersistentMemorySize != that1.DevicePersistentMemorySize {
		return false
	}
	if len(this.DevicePersistentTensorAllocIds) != len(that1.DevicePersistentTensorAllocIds) {
		return false
	}
	for i := range this.DevicePersistentTensorAllocIds {
		if this.DevicePersistentTensorAllocIds[i] != that1.DevicePersistentTensorAllocIds[i] {
			return false
		}
	}
	return true
}
func (this *NodeExecStats) VerboseEqual(that interface{}) error {
	if that == nil {
		if this == nil {
			return nil
		}
		return fmt.Errorf("that == nil && this != nil")
	}

	that1, ok := that.(*NodeExecStats)
	if !ok {
		that2, ok := that.(NodeExecStats)
		if ok {
			that1 = &that2
		} else {
			return fmt.Errorf("that is not of type *NodeExecStats")
		}
	}
	if that1 == nil {
		if this == nil {
			return nil
		}
		return fmt.Errorf("that is type *NodeExecStats but is nil && this != nil")
	} else if this == nil {
		return fmt.Errorf("that is type *NodeExecStats but is not nil && this == nil")
	}
	if this.NodeName != that1.NodeName {
		return fmt.Errorf("NodeName this(%v) Not Equal that(%v)", this.NodeName, that1.NodeName)
	}
	if this.AllStartMicros != that1.AllStartMicros {
		return fmt.Errorf("AllStartMicros this(%v) Not Equal that(%v)", this.AllStartMicros, that1.AllStartMicros)
	}
	if this.OpStartRelMicros != that1.OpStartRelMicros {
		return fmt.Errorf("OpStartRelMicros this(%v) Not Equal that(%v)", this.OpStartRelMicros, that1.OpStartRelMicros)
	}
	if this.OpEndRelMicros != that1.OpEndRelMicros {
		return fmt.Errorf("OpEndRelMicros this(%v) Not Equal that(%v)", this.OpEndRelMicros, that1.OpEndRelMicros)
	}
	if this.AllEndRelMicros != that1.AllEndRelMicros {
		return fmt.Errorf("AllEndRelMicros this(%v) Not Equal that(%v)", this.AllEndRelMicros, that1.AllEndRelMicros)
	}
	if len(this.Memory) != len(that1.Memory) {
		return fmt.Errorf("Memory this(%v) Not Equal that(%v)", len(this.Memory), len(that1.Memory))
	}
	for i := range this.Memory {
		if !this.Memory[i].Equal(that1.Memory[i]) {
			return fmt.Errorf("Memory this[%v](%v) Not Equal that[%v](%v)", i, this.Memory[i], i, that1.Memory[i])
		}
	}
	if len(this.Output) != len(that1.Output) {
		return fmt.Errorf("Output this(%v) Not Equal that(%v)", len(this.Output), len(that1.Output))
	}
	for i := range this.Output {
		if !this.Output[i].Equal(that1.Output[i]) {
			return fmt.Errorf("Output this[%v](%v) Not Equal that[%v](%v)", i, this.Output[i], i, that1.Output[i])
		}
	}
	if this.TimelineLabel != that1.TimelineLabel {
		return fmt.Errorf("TimelineLabel this(%v) Not Equal that(%v)", this.TimelineLabel, that1.TimelineLabel)
	}
	if this.ScheduledMicros != that1.ScheduledMicros {
		return fmt.Errorf("ScheduledMicros this(%v) Not Equal that(%v)", this.ScheduledMicros, that1.ScheduledMicros)
	}
	if this.ThreadId != that1.ThreadId {
		return fmt.Errorf("ThreadId this(%v) Not Equal that(%v)", this.ThreadId, that1.ThreadId)
	}
	if len(this.ReferencedTensor) != len(that1.ReferencedTensor) {
		return fmt.Errorf("ReferencedTensor this(%v) Not Equal that(%v)", len(this.ReferencedTensor), len(that1.ReferencedTensor))
	}
	for i := range this.ReferencedTensor {
		if !this.ReferencedTensor[i].Equal(that1.ReferencedTensor[i]) {
			return fmt.Errorf("ReferencedTensor this[%v](%v) Not Equal that[%v](%v)", i, this.ReferencedTensor[i], i, that1.ReferencedTensor[i])
		}
	}
	if !this.MemoryStats.Equal(that1.MemoryStats) {
		return fmt.Errorf("MemoryStats this(%v) Not Equal that(%v)", this.MemoryStats, that1.MemoryStats)
	}
	if this.AllStartNanos != that1.AllStartNanos {
		return fmt.Errorf("AllStartNanos this(%v) Not Equal that(%v)", this.AllStartNanos, that1.AllStartNanos)
	}
	if this.OpStartRelNanos != that1.OpStartRelNanos {
		return fmt.Errorf("OpStartRelNanos this(%v) Not Equal that(%v)", this.OpStartRelNanos, that1.OpStartRelNanos)
	}
	if this.OpEndRelNanos != that1.OpEndRelNanos {
		return fmt.Errorf("OpEndRelNanos this(%v) Not Equal that(%v)", this.OpEndRelNanos, that1.OpEndRelNanos)
	}
	if this.AllEndRelNanos != that1.AllEndRelNanos {
		return fmt.Errorf("AllEndRelNanos this(%v) Not Equal that(%v)", this.AllEndRelNanos, that1.AllEndRelNanos)
	}
	if this.ScheduledNanos != that1.ScheduledNanos {
		return fmt.Errorf("ScheduledNanos this(%v) Not Equal that(%v)", this.ScheduledNanos, that1.ScheduledNanos)
	}
	return nil
}
func (this *NodeExecStats) Equal(that interface{}) bool {
	if that == nil {
		return this == nil
	}

	that1, ok := that.(*NodeExecStats)
	if !ok {
		that2, ok := that.(NodeExecStats)
		if ok {
			that1 = &that2
		} else {
			return false
		}
	}
	if that1 == nil {
		return this == nil
	} else if this == nil {
		return false
	}
	if this.NodeName != that1.NodeName {
		return false
	}
	if this.AllStartMicros != that1.AllStartMicros {
		return false
	}
	if this.OpStartRelMicros != that1.OpStartRelMicros {
		return false
	}
	if this.OpEndRelMicros != that1.OpEndRelMicros {
		return false
	}
	if this.AllEndRelMicros != that1.AllEndRelMicros {
		return false
	}
	if len(this.Memory) != len(that1.Memory) {
		return false
	}
	for i := range this.Memory {
		if !this.Memory[i].Equal(that1.Memory[i]) {
			return false
		}
	}
	if len(this.Output) != len(that1.Output) {
		return false
	}
	for i := range this.Output {
		if !this.Output[i].Equal(that1.Output[i]) {
			return false
		}
	}
	if this.TimelineLabel != that1.TimelineLabel {
		return false
	}
	if this.ScheduledMicros != that1.ScheduledMicros {
		return false
	}
	if this.ThreadId != that1.ThreadId {
		return false
	}
	if len(this.ReferencedTensor) != len(that1.ReferencedTensor) {
		return false
	}
	for i := range this.ReferencedTensor {
		if !this.ReferencedTensor[i].Equal(that1.ReferencedTensor[i]) {
			return false
		}
	}
	if !this.MemoryStats.Equal(that1.MemoryStats) {
		return false
	}
	if this.AllStartNanos != that1.AllStartNanos {
		return false
	}
	if this.OpStartRelNanos != that1.OpStartRelNanos {
		return false
	}
	if this.OpEndRelNanos != that1.OpEndRelNanos {
		return false
	}
	if this.AllEndRelNanos != that1.AllEndRelNanos {
		return false
	}
	if this.ScheduledNanos != that1.ScheduledNanos {
		return false
	}
	return true
}
func (this *DeviceStepStats) VerboseEqual(that interface{}) error {
	if that == nil {
		if this == nil {
			return nil
		}
		return fmt.Errorf("that == nil && this != nil")
	}

	that1, ok := that.(*DeviceStepStats)
	if !ok {
		that2, ok := that.(DeviceStepStats)
		if ok {
			that1 = &that2
		} else {
			return fmt.Errorf("that is not of type *DeviceStepStats")
		}
	}
	if that1 == nil {
		if this == nil {
			return nil
		}
		return fmt.Errorf("that is type *DeviceStepStats but is nil && this != nil")
	} else if this == nil {
		return fmt.Errorf("that is type *DeviceStepStats but is not nil && this == nil")
	}
	if this.Device != that1.Device {
		return fmt.Errorf("Device this(%v) Not Equal that(%v)", this.Device, that1.Device)
	}
	if len(this.NodeStats) != len(that1.NodeStats) {
		return fmt.Errorf("NodeStats this(%v) Not Equal that(%v)", len(this.NodeStats), len(that1.NodeStats))
	}
	for i := range this.NodeStats {
		if !this.NodeStats[i].Equal(that1.NodeStats[i]) {
			return fmt.Errorf("NodeStats this[%v](%v) Not Equal that[%v](%v)", i, this.NodeStats[i], i, that1.NodeStats[i])
		}
	}
	return nil
}
func (this *DeviceStepStats) Equal(that interface{}) bool {
	if that == nil {
		return this == nil
	}

	that1, ok := that.(*DeviceStepStats)
	if !ok {
		that2, ok := that.(DeviceStepStats)
		if ok {
			that1 = &that2
		} else {
			return false
		}
	}
	if that1 == nil {
		return this == nil
	} else if this == nil {
		return false
	}
	if this.Device != that1.Device {
		return false
	}
	if len(this.NodeStats) != len(that1.NodeStats) {
		return false
	}
	for i := range this.NodeStats {
		if !this.NodeStats[i].Equal(that1.NodeStats[i]) {
			return false
		}
	}
	return true
}
func (this *StepStats) VerboseEqual(that interface{}) error {
	if that == nil {
		if this == nil {
			return nil
		}
		return fmt.Errorf("that == nil && this != nil")
	}

	that1, ok := that.(*StepStats)
	if !ok {
		that2, ok := that.(StepStats)
		if ok {
			that1 = &that2
		} else {
			return fmt.Errorf("that is not of type *StepStats")
		}
	}
	if that1 == nil {
		if this == nil {
			return nil
		}
		return fmt.Errorf("that is type *StepStats but is nil && this != nil")
	} else if this == nil {
		return fmt.Errorf("that is type *StepStats but is not nil && this == nil")
	}
	if len(this.DevStats) != len(that1.DevStats) {
		return fmt.Errorf("DevStats this(%v) Not Equal that(%v)", len(this.DevStats), len(that1.DevStats))
	}
	for i := range this.DevStats {
		if !this.DevStats[i].Equal(that1.DevStats[i]) {
			return fmt.Errorf("DevStats this[%v](%v) Not Equal that[%v](%v)", i, this.DevStats[i], i, that1.DevStats[i])
		}
	}
	return nil
}
func (this *StepStats) Equal(that interface{}) bool {
	if that == nil {
		return this == nil
	}

	that1, ok := that.(*StepStats)
	if !ok {
		that2, ok := that.(StepStats)
		if ok {
			that1 = &that2
		} else {
			return false
		}
	}
	if that1 == nil {
		return this == nil
	} else if this == nil {
		return false
	}
	if len(this.DevStats) != len(that1.DevStats) {
		return false
	}
	for i := range this.DevStats {
		if !this.DevStats[i].Equal(that1.DevStats[i]) {
			return false
		}
	}
	return true
}
func (this *AllocationRecord) GoString() string {
	if this == nil {
		return "nil"
	}
	s := make([]string, 0, 6)
	s = append(s, "&framework.AllocationRecord{")
	s = append(s, "AllocMicros: "+fmt.Sprintf("%#v", this.AllocMicros)+",\n")
	s = append(s, "AllocBytes: "+fmt.Sprintf("%#v", this.AllocBytes)+",\n")
	s = append(s, "}")
	return strings.Join(s, "")
}
func (this *AllocatorMemoryUsed) GoString() string {
	if this == nil {
		return "nil"
	}
	s := make([]string, 0, 10)
	s = append(s, "&framework.AllocatorMemoryUsed{")
	s = append(s, "AllocatorName: "+fmt.Sprintf("%#v", this.AllocatorName)+",\n")
	s = append(s, "TotalBytes: "+fmt.Sprintf("%#v", this.TotalBytes)+",\n")
	s = append(s, "PeakBytes: "+fmt.Sprintf("%#v", this.PeakBytes)+",\n")
	s = append(s, "LiveBytes: "+fmt.Sprintf("%#v", this.LiveBytes)+",\n")
	if this.AllocationRecords != nil {
		s = append(s, "AllocationRecords: "+fmt.Sprintf("%#v", this.AllocationRecords)+",\n")
	}
	s = append(s, "AllocatorBytesInUse: "+fmt.Sprintf("%#v", this.AllocatorBytesInUse)+",\n")
	s = append(s, "}")
	return strings.Join(s, "")
}
func (this *NodeOutput) GoString() string {
	if this == nil {
		return "nil"
	}
	s := make([]string, 0, 6)
	s = append(s, "&framework.NodeOutput{")
	s = append(s, "Slot: "+fmt.Sprintf("%#v", this.Slot)+",\n")
	if this.TensorDescription != nil {
		s = append(s, "TensorDescription: "+fmt.Sprintf("%#v", this.TensorDescription)+",\n")
	}
	s = append(s, "}")
	return strings.Join(s, "")
}
func (this *MemoryStats) GoString() string {
	if this == nil {
		return "nil"
	}
	s := make([]string, 0, 10)
	s = append(s, "&framework.MemoryStats{")
	s = append(s, "TempMemorySize: "+fmt.Sprintf("%#v", this.TempMemorySize)+",\n")
	s = append(s, "PersistentMemorySize: "+fmt.Sprintf("%#v", this.PersistentMemorySize)+",\n")
	s = append(s, "PersistentTensorAllocIds: "+fmt.Sprintf("%#v", this.PersistentTensorAllocIds)+",\n")
	s = append(s, "DeviceTempMemorySize: "+fmt.Sprintf("%#v", this.DeviceTempMemorySize)+",\n")
	s = append(s, "DevicePersistentMemorySize: "+fmt.Sprintf("%#v", this.DevicePersistentMemorySize)+",\n")
	s = append(s, "DevicePersistentTensorAllocIds: "+fmt.Sprintf("%#v", this.DevicePersistentTensorAllocIds)+",\n")
	s = append(s, "}")
	return strings.Join(s, "")
}
func (this *NodeExecStats) GoString() string {
	if this == nil {
		return "nil"
	}
	s := make([]string, 0, 21)
	s = append(s, "&framework.NodeExecStats{")
	s = append(s, "NodeName: "+fmt.Sprintf("%#v", this.NodeName)+",\n")
	s = append(s, "AllStartMicros: "+fmt.Sprintf("%#v", this.AllStartMicros)+",\n")
	s = append(s, "OpStartRelMicros: "+fmt.Sprintf("%#v", this.OpStartRelMicros)+",\n")
	s = append(s, "OpEndRelMicros: "+fmt.Sprintf("%#v", this.OpEndRelMicros)+",\n")
	s = append(s, "AllEndRelMicros: "+fmt.Sprintf("%#v", this.AllEndRelMicros)+",\n")
	if this.Memory != nil {
		s = append(s, "Memory: "+fmt.Sprintf("%#v", this.Memory)+",\n")
	}
	if this.Output != nil {
		s = append(s, "Output: "+fmt.Sprintf("%#v", this.Output)+",\n")
	}
	s = append(s, "TimelineLabel: "+fmt.Sprintf("%#v", this.TimelineLabel)+",\n")
	s = append(s, "ScheduledMicros: "+fmt.Sprintf("%#v", this.ScheduledMicros)+",\n")
	s = append(s, "ThreadId: "+fmt.Sprintf("%#v", this.ThreadId)+",\n")
	if this.ReferencedTensor != nil {
		s = append(s, "ReferencedTensor: "+fmt.Sprintf("%#v", this.ReferencedTensor)+",\n")
	}
	if this.MemoryStats != nil {
		s = append(s, "MemoryStats: "+fmt.Sprintf("%#v", this.MemoryStats)+",\n")
	}
	s = append(s, "AllStartNanos: "+fmt.Sprintf("%#v", this.AllStartNanos)+",\n")
	s = append(s, "OpStartRelNanos: "+fmt.Sprintf("%#v", this.OpStartRelNanos)+",\n")
	s = append(s, "OpEndRelNanos: "+fmt.Sprintf("%#v", this.OpEndRelNanos)+",\n")
	s = append(s, "AllEndRelNanos: "+fmt.Sprintf("%#v", this.AllEndRelNanos)+",\n")
	s = append(s, "ScheduledNanos: "+fmt.Sprintf("%#v", this.ScheduledNanos)+",\n")
	s = append(s, "}")
	return strings.Join(s, "")
}
func (this *DeviceStepStats) GoString() string {
	if this == nil {
		return "nil"
	}
	s := make([]string, 0, 6)
	s = append(s, "&framework.DeviceStepStats{")
	s = append(s, "Device: "+fmt.Sprintf("%#v", this.Device)+",\n")
	if this.NodeStats != nil {
		s = append(s, "NodeStats: "+fmt.Sprintf("%#v", this.NodeStats)+",\n")
	}
	s = append(s, "}")
	return strings.Join(s, "")
}
func (this *StepStats) GoString() string {
	if this == nil {
		return "nil"
	}
	s := make([]string, 0, 5)
	s = append(s, "&framework.StepStats{")
	if this.DevStats != nil {
		s = append(s, "DevStats: "+fmt.Sprintf("%#v", this.DevStats)+",\n")
	}
	s = append(s, "}")
	return strings.Join(s, "")
}
func valueToGoStringStepStats(v interface{}, typ string) string {
	rv := reflect.ValueOf(v)
	if rv.IsNil() {
		return "nil"
	}
	pv := reflect.Indirect(rv).Interface()
	return fmt.Sprintf("func(v %v) *%v { return &v } ( %#v )", typ, typ, pv)
}
func (m *AllocationRecord) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalTo(dAtA)
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *AllocationRecord) MarshalTo(dAtA []byte) (int, error) {
	var i int
	_ = i
	var l int
	_ = l
	if m.AllocMicros != 0 {
		dAtA[i] = 0x8
		i++
		i = encodeVarintStepStats(dAtA, i, uint64(m.AllocMicros))
	}
	if m.AllocBytes != 0 {
		dAtA[i] = 0x10
		i++
		i = encodeVarintStepStats(dAtA, i, uint64(m.AllocBytes))
	}
	return i, nil
}

func (m *AllocatorMemoryUsed) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalTo(dAtA)
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *AllocatorMemoryUsed) MarshalTo(dAtA []byte) (int, error) {
	var i int
	_ = i
	var l int
	_ = l
	if len(m.AllocatorName) > 0 {
		dAtA[i] = 0xa
		i++
		i = encodeVarintStepStats(dAtA, i, uint64(len(m.AllocatorName)))
		i += copy(dAtA[i:], m.AllocatorName)
	}
	if m.TotalBytes != 0 {
		dAtA[i] = 0x10
		i++
		i = encodeVarintStepStats(dAtA, i, uint64(m.TotalBytes))
	}
	if m.PeakBytes != 0 {
		dAtA[i] = 0x18
		i++
		i = encodeVarintStepStats(dAtA, i, uint64(m.PeakBytes))
	}
	if m.LiveBytes != 0 {
		dAtA[i] = 0x20
		i++
		i = encodeVarintStepStats(dAtA, i, uint64(m.LiveBytes))
	}
	if m.AllocatorBytesInUse != 0 {
		dAtA[i] = 0x28
		i++
		i = encodeVarintStepStats(dAtA, i, uint64(m.AllocatorBytesInUse))
	}
	if len(m.AllocationRecords) > 0 {
		for _, msg := range m.AllocationRecords {
			dAtA[i] = 0x32
			i++
			i = encodeVarintStepStats(dAtA, i, uint64(msg.Size()))
			n, err := msg.MarshalTo(dAtA[i:])
			if err != nil {
				return 0, err
			}
			i += n
		}
	}
	return i, nil
}

func (m *NodeOutput) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalTo(dAtA)
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *NodeOutput) MarshalTo(dAtA []byte) (int, error) {
	var i int
	_ = i
	var l int
	_ = l
	if m.Slot != 0 {
		dAtA[i] = 0x8
		i++
		i = encodeVarintStepStats(dAtA, i, uint64(m.Slot))
	}
	if m.TensorDescription != nil {
		dAtA[i] = 0x1a
		i++
		i = encodeVarintStepStats(dAtA, i, uint64(m.TensorDescription.Size()))
		n1, err := m.TensorDescription.MarshalTo(dAtA[i:])
		if err != nil {
			return 0, err
		}
		i += n1
	}
	return i, nil
}

func (m *MemoryStats) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalTo(dAtA)
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *MemoryStats) MarshalTo(dAtA []byte) (int, error) {
	var i int
	_ = i
	var l int
	_ = l
	if m.TempMemorySize != 0 {
		dAtA[i] = 0x8
		i++
		i = encodeVarintStepStats(dAtA, i, uint64(m.TempMemorySize))
	}
	if m.DeviceTempMemorySize != 0 {
		dAtA[i] = 0x10
		i++
		i = encodeVarintStepStats(dAtA, i, uint64(m.DeviceTempMemorySize))
	}
	if m.PersistentMemorySize != 0 {
		dAtA[i] = 0x18
		i++
		i = encodeVarintStepStats(dAtA, i, uint64(m.PersistentMemorySize))
	}
	if m.DevicePersistentMemorySize != 0 {
		dAtA[i] = 0x20
		i++
		i = encodeVarintStepStats(dAtA, i, uint64(m.DevicePersistentMemorySize))
	}
	if len(m.PersistentTensorAllocIds) > 0 {
		dAtA3 := make([]byte, len(m.PersistentTensorAllocIds)*10)
		var j2 int
		for _, num1 := range m.PersistentTensorAllocIds {
			num := uint64(num1)
			for num >= 1<<7 {
				dAtA3[j2] = uint8(uint64(num)&0x7f | 0x80)
				num >>= 7
				j2++
			}
			dAtA3[j2] = uint8(num)
			j2++
		}
		dAtA[i] = 0x2a
		i++
		i = encodeVarintStepStats(dAtA, i, uint64(j2))
		i += copy(dAtA[i:], dAtA3[:j2])
	}
	if len(m.DevicePersistentTensorAllocIds) > 0 {
		dAtA5 := make([]byte, len(m.DevicePersistentTensorAllocIds)*10)
		var j4 int
		for _, num1 := range m.DevicePersistentTensorAllocIds {
			num := uint64(num1)
			for num >= 1<<7 {
				dAtA5[j4] = uint8(uint64(num)&0x7f | 0x80)
				num >>= 7
				j4++
			}
			dAtA5[j4] = uint8(num)
			j4++
		}
		dAtA[i] = 0x32
		i++
		i = encodeVarintStepStats(dAtA, i, uint64(j4))
		i += copy(dAtA[i:], dAtA5[:j4])
	}
	return i, nil
}

func (m *NodeExecStats) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalTo(dAtA)
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *NodeExecStats) MarshalTo(dAtA []byte) (int, error) {
	var i int
	_ = i
	var l int
	_ = l
	if len(m.NodeName) > 0 {
		dAtA[i] = 0xa
		i++
		i = encodeVarintStepStats(dAtA, i, uint64(len(m.NodeName)))
		i += copy(dAtA[i:], m.NodeName)
	}
	if m.AllStartMicros != 0 {
		dAtA[i] = 0x10
		i++
		i = encodeVarintStepStats(dAtA, i, uint64(m.AllStartMicros))
	}
	if m.OpStartRelMicros != 0 {
		dAtA[i] = 0x18
		i++
		i = encodeVarintStepStats(dAtA, i, uint64(m.OpStartRelMicros))
	}
	if m.OpEndRelMicros != 0 {
		dAtA[i] = 0x20
		i++
		i = encodeVarintStepStats(dAtA, i, uint64(m.OpEndRelMicros))
	}
	if m.AllEndRelMicros != 0 {
		dAtA[i] = 0x28
		i++
		i = encodeVarintStepStats(dAtA, i, uint64(m.AllEndRelMicros))
	}
	if len(m.Memory) > 0 {
		for _, msg := range m.Memory {
			dAtA[i] = 0x32
			i++
			i = encodeVarintStepStats(dAtA, i, uint64(msg.Size()))
			n, err := msg.MarshalTo(dAtA[i:])
			if err != nil {
				return 0, err
			}
			i += n
		}
	}
	if len(m.Output) > 0 {
		for _, msg := range m.Output {
			dAtA[i] = 0x3a
			i++
			i = encodeVarintStepStats(dAtA, i, uint64(msg.Size()))
			n, err := msg.MarshalTo(dAtA[i:])
			if err != nil {
				return 0, err
			}
			i += n
		}
	}
	if len(m.TimelineLabel) > 0 {
		dAtA[i] = 0x42
		i++
		i = encodeVarintStepStats(dAtA, i, uint64(len(m.TimelineLabel)))
		i += copy(dAtA[i:], m.TimelineLabel)
	}
	if m.ScheduledMicros != 0 {
		dAtA[i] = 0x48
		i++
		i = encodeVarintStepStats(dAtA, i, uint64(m.ScheduledMicros))
	}
	if m.ThreadId != 0 {
		dAtA[i] = 0x50
		i++
		i = encodeVarintStepStats(dAtA, i, uint64(m.ThreadId))
	}
	if len(m.ReferencedTensor) > 0 {
		for _, msg := range m.ReferencedTensor {
			dAtA[i] = 0x5a
			i++
			i = encodeVarintStepStats(dAtA, i, uint64(msg.Size()))
			n, err := msg.MarshalTo(dAtA[i:])
			if err != nil {
				return 0, err
			}
			i += n
		}
	}
	if m.MemoryStats != nil {
		dAtA[i] = 0x62
		i++
		i = encodeVarintStepStats(dAtA, i, uint64(m.MemoryStats.Size()))
		n6, err := m.MemoryStats.MarshalTo(dAtA[i:])
		if err != nil {
			return 0, err
		}
		i += n6
	}
	if m.AllStartNanos != 0 {
		dAtA[i] = 0x68
		i++
		i = encodeVarintStepStats(dAtA, i, uint64(m.AllStartNanos))
	}
	if m.OpStartRelNanos != 0 {
		dAtA[i] = 0x70
		i++
		i = encodeVarintStepStats(dAtA, i, uint64(m.OpStartRelNanos))
	}
	if m.OpEndRelNanos != 0 {
		dAtA[i] = 0x78
		i++
		i = encodeVarintStepStats(dAtA, i, uint64(m.OpEndRelNanos))
	}
	if m.AllEndRelNanos != 0 {
		dAtA[i] = 0x80
		i++
		dAtA[i] = 0x1
		i++
		i = encodeVarintStepStats(dAtA, i, uint64(m.AllEndRelNanos))
	}
	if m.ScheduledNanos != 0 {
		dAtA[i] = 0x88
		i++
		dAtA[i] = 0x1
		i++
		i = encodeVarintStepStats(dAtA, i, uint64(m.ScheduledNanos))
	}
	return i, nil
}

func (m *DeviceStepStats) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalTo(dAtA)
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *DeviceStepStats) MarshalTo(dAtA []byte) (int, error) {
	var i int
	_ = i
	var l int
	_ = l
	if len(m.Device) > 0 {
		dAtA[i] = 0xa
		i++
		i = encodeVarintStepStats(dAtA, i, uint64(len(m.Device)))
		i += copy(dAtA[i:], m.Device)
	}
	if len(m.NodeStats) > 0 {
		for _, msg := range m.NodeStats {
			dAtA[i] = 0x12
			i++
			i = encodeVarintStepStats(dAtA, i, uint64(msg.Size()))
			n, err := msg.MarshalTo(dAtA[i:])
			if err != nil {
				return 0, err
			}
			i += n
		}
	}
	return i, nil
}

func (m *StepStats) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalTo(dAtA)
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *StepStats) MarshalTo(dAtA []byte) (int, error) {
	var i int
	_ = i
	var l int
	_ = l
	if len(m.DevStats) > 0 {
		for _, msg := range m.DevStats {
			dAtA[i] = 0xa
			i++
			i = encodeVarintStepStats(dAtA, i, uint64(msg.Size()))
			n, err := msg.MarshalTo(dAtA[i:])
			if err != nil {
				return 0, err
			}
			i += n
		}
	}
	return i, nil
}

func encodeVarintStepStats(dAtA []byte, offset int, v uint64) int {
	for v >= 1<<7 {
		dAtA[offset] = uint8(v&0x7f | 0x80)
		v >>= 7
		offset++
	}
	dAtA[offset] = uint8(v)
	return offset + 1
}
func NewPopulatedAllocationRecord(r randyStepStats, easy bool) *AllocationRecord {
	this := &AllocationRecord{}
	this.AllocMicros = int64(r.Int63())
	if r.Intn(2) == 0 {
		this.AllocMicros *= -1
	}
	this.AllocBytes = int64(r.Int63())
	if r.Intn(2) == 0 {
		this.AllocBytes *= -1
	}
	if !easy && r.Intn(10) != 0 {
	}
	return this
}

func NewPopulatedAllocatorMemoryUsed(r randyStepStats, easy bool) *AllocatorMemoryUsed {
	this := &AllocatorMemoryUsed{}
	this.AllocatorName = string(randStringStepStats(r))
	this.TotalBytes = int64(r.Int63())
	if r.Intn(2) == 0 {
		this.TotalBytes *= -1
	}
	this.PeakBytes = int64(r.Int63())
	if r.Intn(2) == 0 {
		this.PeakBytes *= -1
	}
	this.LiveBytes = int64(r.Int63())
	if r.Intn(2) == 0 {
		this.LiveBytes *= -1
	}
	this.AllocatorBytesInUse = int64(r.Int63())
	if r.Intn(2) == 0 {
		this.AllocatorBytesInUse *= -1
	}
	if r.Intn(10) != 0 {
		v1 := r.Intn(5)
		this.AllocationRecords = make([]*AllocationRecord, v1)
		for i := 0; i < v1; i++ {
			this.AllocationRecords[i] = NewPopulatedAllocationRecord(r, easy)
		}
	}
	if !easy && r.Intn(10) != 0 {
	}
	return this
}

func NewPopulatedNodeOutput(r randyStepStats, easy bool) *NodeOutput {
	this := &NodeOutput{}
	this.Slot = int32(r.Int31())
	if r.Intn(2) == 0 {
		this.Slot *= -1
	}
	if r.Intn(10) != 0 {
		this.TensorDescription = NewPopulatedTensorDescription(r, easy)
	}
	if !easy && r.Intn(10) != 0 {
	}
	return this
}

func NewPopulatedMemoryStats(r randyStepStats, easy bool) *MemoryStats {
	this := &MemoryStats{}
	this.TempMemorySize = int64(r.Int63())
	if r.Intn(2) == 0 {
		this.TempMemorySize *= -1
	}
	this.DeviceTempMemorySize = int64(r.Int63())
	if r.Intn(2) == 0 {
		this.DeviceTempMemorySize *= -1
	}
	this.PersistentMemorySize = int64(r.Int63())
	if r.Intn(2) == 0 {
		this.PersistentMemorySize *= -1
	}
	this.DevicePersistentMemorySize = int64(r.Int63())
	if r.Intn(2) == 0 {
		this.DevicePersistentMemorySize *= -1
	}
	v2 := r.Intn(10)
	this.PersistentTensorAllocIds = make([]int64, v2)
	for i := 0; i < v2; i++ {
		this.PersistentTensorAllocIds[i] = int64(r.Int63())
		if r.Intn(2) == 0 {
			this.PersistentTensorAllocIds[i] *= -1
		}
	}
	v3 := r.Intn(10)
	this.DevicePersistentTensorAllocIds = make([]int64, v3)
	for i := 0; i < v3; i++ {
		this.DevicePersistentTensorAllocIds[i] = int64(r.Int63())
		if r.Intn(2) == 0 {
			this.DevicePersistentTensorAllocIds[i] *= -1
		}
	}
	if !easy && r.Intn(10) != 0 {
	}
	return this
}

func NewPopulatedNodeExecStats(r randyStepStats, easy bool) *NodeExecStats {
	this := &NodeExecStats{}
	this.NodeName = string(randStringStepStats(r))
	this.AllStartMicros = int64(r.Int63())
	if r.Intn(2) == 0 {
		this.AllStartMicros *= -1
	}
	this.OpStartRelMicros = int64(r.Int63())
	if r.Intn(2) == 0 {
		this.OpStartRelMicros *= -1
	}
	this.OpEndRelMicros = int64(r.Int63())
	if r.Intn(2) == 0 {
		this.OpEndRelMicros *= -1
	}
	this.AllEndRelMicros = int64(r.Int63())
	if r.Intn(2) == 0 {
		this.AllEndRelMicros *= -1
	}
	if r.Intn(10) != 0 {
		v4 := r.Intn(5)
		this.Memory = make([]*AllocatorMemoryUsed, v4)
		for i := 0; i < v4; i++ {
			this.Memory[i] = NewPopulatedAllocatorMemoryUsed(r, easy)
		}
	}
	if r.Intn(10) != 0 {
		v5 := r.Intn(5)
		this.Output = make([]*NodeOutput, v5)
		for i := 0; i < v5; i++ {
			this.Output[i] = NewPopulatedNodeOutput(r, easy)
		}
	}
	this.TimelineLabel = string(randStringStepStats(r))
	this.ScheduledMicros = int64(r.Int63())
	if r.Intn(2) == 0 {
		this.ScheduledMicros *= -1
	}
	this.ThreadId = uint32(r.Uint32())
	if r.Intn(10) != 0 {
		v6 := r.Intn(5)
		this.ReferencedTensor = make([]*AllocationDescription, v6)
		for i := 0; i < v6; i++ {
			this.ReferencedTensor[i] = NewPopulatedAllocationDescription(r, easy)
		}
	}
	if r.Intn(10) != 0 {
		this.MemoryStats = NewPopulatedMemoryStats(r, easy)
	}
	this.AllStartNanos = int64(r.Int63())
	if r.Intn(2) == 0 {
		this.AllStartNanos *= -1
	}
	this.OpStartRelNanos = int64(r.Int63())
	if r.Intn(2) == 0 {
		this.OpStartRelNanos *= -1
	}
	this.OpEndRelNanos = int64(r.Int63())
	if r.Intn(2) == 0 {
		this.OpEndRelNanos *= -1
	}
	this.AllEndRelNanos = int64(r.Int63())
	if r.Intn(2) == 0 {
		this.AllEndRelNanos *= -1
	}
	this.ScheduledNanos = int64(r.Int63())
	if r.Intn(2) == 0 {
		this.ScheduledNanos *= -1
	}
	if !easy && r.Intn(10) != 0 {
	}
	return this
}

func NewPopulatedDeviceStepStats(r randyStepStats, easy bool) *DeviceStepStats {
	this := &DeviceStepStats{}
	this.Device = string(randStringStepStats(r))
	if r.Intn(10) != 0 {
		v7 := r.Intn(5)
		this.NodeStats = make([]*NodeExecStats, v7)
		for i := 0; i < v7; i++ {
			this.NodeStats[i] = NewPopulatedNodeExecStats(r, easy)
		}
	}
	if !easy && r.Intn(10) != 0 {
	}
	return this
}

func NewPopulatedStepStats(r randyStepStats, easy bool) *StepStats {
	this := &StepStats{}
	if r.Intn(10) != 0 {
		v8 := r.Intn(5)
		this.DevStats = make([]*DeviceStepStats, v8)
		for i := 0; i < v8; i++ {
			this.DevStats[i] = NewPopulatedDeviceStepStats(r, easy)
		}
	}
	if !easy && r.Intn(10) != 0 {
	}
	return this
}

type randyStepStats interface {
	Float32() float32
	Float64() float64
	Int63() int64
	Int31() int32
	Uint32() uint32
	Intn(n int) int
}

func randUTF8RuneStepStats(r randyStepStats) rune {
	ru := r.Intn(62)
	if ru < 10 {
		return rune(ru + 48)
	} else if ru < 36 {
		return rune(ru + 55)
	}
	return rune(ru + 61)
}
func randStringStepStats(r randyStepStats) string {
	v9 := r.Intn(100)
	tmps := make([]rune, v9)
	for i := 0; i < v9; i++ {
		tmps[i] = randUTF8RuneStepStats(r)
	}
	return string(tmps)
}
func randUnrecognizedStepStats(r randyStepStats, maxFieldNumber int) (dAtA []byte) {
	l := r.Intn(5)
	for i := 0; i < l; i++ {
		wire := r.Intn(4)
		if wire == 3 {
			wire = 5
		}
		fieldNumber := maxFieldNumber + r.Intn(100)
		dAtA = randFieldStepStats(dAtA, r, fieldNumber, wire)
	}
	return dAtA
}
func randFieldStepStats(dAtA []byte, r randyStepStats, fieldNumber int, wire int) []byte {
	key := uint32(fieldNumber)<<3 | uint32(wire)
	switch wire {
	case 0:
		dAtA = encodeVarintPopulateStepStats(dAtA, uint64(key))
		v10 := r.Int63()
		if r.Intn(2) == 0 {
			v10 *= -1
		}
		dAtA = encodeVarintPopulateStepStats(dAtA, uint64(v10))
	case 1:
		dAtA = encodeVarintPopulateStepStats(dAtA, uint64(key))
		dAtA = append(dAtA, byte(r.Intn(256)), byte(r.Intn(256)), byte(r.Intn(256)), byte(r.Intn(256)), byte(r.Intn(256)), byte(r.Intn(256)), byte(r.Intn(256)), byte(r.Intn(256)))
	case 2:
		dAtA = encodeVarintPopulateStepStats(dAtA, uint64(key))
		ll := r.Intn(100)
		dAtA = encodeVarintPopulateStepStats(dAtA, uint64(ll))
		for j := 0; j < ll; j++ {
			dAtA = append(dAtA, byte(r.Intn(256)))
		}
	default:
		dAtA = encodeVarintPopulateStepStats(dAtA, uint64(key))
		dAtA = append(dAtA, byte(r.Intn(256)), byte(r.Intn(256)), byte(r.Intn(256)), byte(r.Intn(256)))
	}
	return dAtA
}
func encodeVarintPopulateStepStats(dAtA []byte, v uint64) []byte {
	for v >= 1<<7 {
		dAtA = append(dAtA, uint8(uint64(v)&0x7f|0x80))
		v >>= 7
	}
	dAtA = append(dAtA, uint8(v))
	return dAtA
}
func (m *AllocationRecord) Size() (n int) {
	var l int
	_ = l
	if m.AllocMicros != 0 {
		n += 1 + sovStepStats(uint64(m.AllocMicros))
	}
	if m.AllocBytes != 0 {
		n += 1 + sovStepStats(uint64(m.AllocBytes))
	}
	return n
}

func (m *AllocatorMemoryUsed) Size() (n int) {
	var l int
	_ = l
	l = len(m.AllocatorName)
	if l > 0 {
		n += 1 + l + sovStepStats(uint64(l))
	}
	if m.TotalBytes != 0 {
		n += 1 + sovStepStats(uint64(m.TotalBytes))
	}
	if m.PeakBytes != 0 {
		n += 1 + sovStepStats(uint64(m.PeakBytes))
	}
	if m.LiveBytes != 0 {
		n += 1 + sovStepStats(uint64(m.LiveBytes))
	}
	if m.AllocatorBytesInUse != 0 {
		n += 1 + sovStepStats(uint64(m.AllocatorBytesInUse))
	}
	if len(m.AllocationRecords) > 0 {
		for _, e := range m.AllocationRecords {
			l = e.Size()
			n += 1 + l + sovStepStats(uint64(l))
		}
	}
	return n
}

func (m *NodeOutput) Size() (n int) {
	var l int
	_ = l
	if m.Slot != 0 {
		n += 1 + sovStepStats(uint64(m.Slot))
	}
	if m.TensorDescription != nil {
		l = m.TensorDescription.Size()
		n += 1 + l + sovStepStats(uint64(l))
	}
	return n
}

func (m *MemoryStats) Size() (n int) {
	var l int
	_ = l
	if m.TempMemorySize != 0 {
		n += 1 + sovStepStats(uint64(m.TempMemorySize))
	}
	if m.DeviceTempMemorySize != 0 {
		n += 1 + sovStepStats(uint64(m.DeviceTempMemorySize))
	}
	if m.PersistentMemorySize != 0 {
		n += 1 + sovStepStats(uint64(m.PersistentMemorySize))
	}
	if m.DevicePersistentMemorySize != 0 {
		n += 1 + sovStepStats(uint64(m.DevicePersistentMemorySize))
	}
	if len(m.PersistentTensorAllocIds) > 0 {
		l = 0
		for _, e := range m.PersistentTensorAllocIds {
			l += sovStepStats(uint64(e))
		}
		n += 1 + sovStepStats(uint64(l)) + l
	}
	if len(m.DevicePersistentTensorAllocIds) > 0 {
		l = 0
		for _, e := range m.DevicePersistentTensorAllocIds {
			l += sovStepStats(uint64(e))
		}
		n += 1 + sovStepStats(uint64(l)) + l
	}
	return n
}

func (m *NodeExecStats) Size() (n int) {
	var l int
	_ = l
	l = len(m.NodeName)
	if l > 0 {
		n += 1 + l + sovStepStats(uint64(l))
	}
	if m.AllStartMicros != 0 {
		n += 1 + sovStepStats(uint64(m.AllStartMicros))
	}
	if m.OpStartRelMicros != 0 {
		n += 1 + sovStepStats(uint64(m.OpStartRelMicros))
	}
	if m.OpEndRelMicros != 0 {
		n += 1 + sovStepStats(uint64(m.OpEndRelMicros))
	}
	if m.AllEndRelMicros != 0 {
		n += 1 + sovStepStats(uint64(m.AllEndRelMicros))
	}
	if len(m.Memory) > 0 {
		for _, e := range m.Memory {
			l = e.Size()
			n += 1 + l + sovStepStats(uint64(l))
		}
	}
	if len(m.Output) > 0 {
		for _, e := range m.Output {
			l = e.Size()
			n += 1 + l + sovStepStats(uint64(l))
		}
	}
	l = len(m.TimelineLabel)
	if l > 0 {
		n += 1 + l + sovStepStats(uint64(l))
	}
	if m.ScheduledMicros != 0 {
		n += 1 + sovStepStats(uint64(m.ScheduledMicros))
	}
	if m.ThreadId != 0 {
		n += 1 + sovStepStats(uint64(m.ThreadId))
	}
	if len(m.ReferencedTensor) > 0 {
		for _, e := range m.ReferencedTensor {
			l = e.Size()
			n += 1 + l + sovStepStats(uint64(l))
		}
	}
	if m.MemoryStats != nil {
		l = m.MemoryStats.Size()
		n += 1 + l + sovStepStats(uint64(l))
	}
	if m.AllStartNanos != 0 {
		n += 1 + sovStepStats(uint64(m.AllStartNanos))
	}
	if m.OpStartRelNanos != 0 {
		n += 1 + sovStepStats(uint64(m.OpStartRelNanos))
	}
	if m.OpEndRelNanos != 0 {
		n += 1 + sovStepStats(uint64(m.OpEndRelNanos))
	}
	if m.AllEndRelNanos != 0 {
		n += 2 + sovStepStats(uint64(m.AllEndRelNanos))
	}
	if m.ScheduledNanos != 0 {
		n += 2 + sovStepStats(uint64(m.ScheduledNanos))
	}
	return n
}

func (m *DeviceStepStats) Size() (n int) {
	var l int
	_ = l
	l = len(m.Device)
	if l > 0 {
		n += 1 + l + sovStepStats(uint64(l))
	}
	if len(m.NodeStats) > 0 {
		for _, e := range m.NodeStats {
			l = e.Size()
			n += 1 + l + sovStepStats(uint64(l))
		}
	}
	return n
}

func (m *StepStats) Size() (n int) {
	var l int
	_ = l
	if len(m.DevStats) > 0 {
		for _, e := range m.DevStats {
			l = e.Size()
			n += 1 + l + sovStepStats(uint64(l))
		}
	}
	return n
}

func sovStepStats(x uint64) (n int) {
	for {
		n++
		x >>= 7
		if x == 0 {
			break
		}
	}
	return n
}
func sozStepStats(x uint64) (n int) {
	return sovStepStats(uint64((x << 1) ^ uint64((int64(x) >> 63))))
}
func (this *AllocationRecord) String() string {
	if this == nil {
		return "nil"
	}
	s := strings.Join([]string{`&AllocationRecord{`,
		`AllocMicros:` + fmt.Sprintf("%v", this.AllocMicros) + `,`,
		`AllocBytes:` + fmt.Sprintf("%v", this.AllocBytes) + `,`,
		`}`,
	}, "")
	return s
}
func (this *AllocatorMemoryUsed) String() string {
	if this == nil {
		return "nil"
	}
	s := strings.Join([]string{`&AllocatorMemoryUsed{`,
		`AllocatorName:` + fmt.Sprintf("%v", this.AllocatorName) + `,`,
		`TotalBytes:` + fmt.Sprintf("%v", this.TotalBytes) + `,`,
		`PeakBytes:` + fmt.Sprintf("%v", this.PeakBytes) + `,`,
		`LiveBytes:` + fmt.Sprintf("%v", this.LiveBytes) + `,`,
		`AllocatorBytesInUse:` + fmt.Sprintf("%v", this.AllocatorBytesInUse) + `,`,
		`AllocationRecords:` + strings.Replace(fmt.Sprintf("%v", this.AllocationRecords), "AllocationRecord", "AllocationRecord", 1) + `,`,
		`}`,
	}, "")
	return s
}
func (this *NodeOutput) String() string {
	if this == nil {
		return "nil"
	}
	s := strings.Join([]string{`&NodeOutput{`,
		`Slot:` + fmt.Sprintf("%v", this.Slot) + `,`,
		`TensorDescription:` + strings.Replace(fmt.Sprintf("%v", this.TensorDescription), "TensorDescription", "TensorDescription", 1) + `,`,
		`}`,
	}, "")
	return s
}
func (this *MemoryStats) String() string {
	if this == nil {
		return "nil"
	}
	s := strings.Join([]string{`&MemoryStats{`,
		`TempMemorySize:` + fmt.Sprintf("%v", this.TempMemorySize) + `,`,
		`DeviceTempMemorySize:` + fmt.Sprintf("%v", this.DeviceTempMemorySize) + `,`,
		`PersistentMemorySize:` + fmt.Sprintf("%v", this.PersistentMemorySize) + `,`,
		`DevicePersistentMemorySize:` + fmt.Sprintf("%v", this.DevicePersistentMemorySize) + `,`,
		`PersistentTensorAllocIds:` + fmt.Sprintf("%v", this.PersistentTensorAllocIds) + `,`,
		`DevicePersistentTensorAllocIds:` + fmt.Sprintf("%v", this.DevicePersistentTensorAllocIds) + `,`,
		`}`,
	}, "")
	return s
}
func (this *NodeExecStats) String() string {
	if this == nil {
		return "nil"
	}
	s := strings.Join([]string{`&NodeExecStats{`,
		`NodeName:` + fmt.Sprintf("%v", this.NodeName) + `,`,
		`AllStartMicros:` + fmt.Sprintf("%v", this.AllStartMicros) + `,`,
		`OpStartRelMicros:` + fmt.Sprintf("%v", this.OpStartRelMicros) + `,`,
		`OpEndRelMicros:` + fmt.Sprintf("%v", this.OpEndRelMicros) + `,`,
		`AllEndRelMicros:` + fmt.Sprintf("%v", this.AllEndRelMicros) + `,`,
		`Memory:` + strings.Replace(fmt.Sprintf("%v", this.Memory), "AllocatorMemoryUsed", "AllocatorMemoryUsed", 1) + `,`,
		`Output:` + strings.Replace(fmt.Sprintf("%v", this.Output), "NodeOutput", "NodeOutput", 1) + `,`,
		`TimelineLabel:` + fmt.Sprintf("%v", this.TimelineLabel) + `,`,
		`ScheduledMicros:` + fmt.Sprintf("%v", this.ScheduledMicros) + `,`,
		`ThreadId:` + fmt.Sprintf("%v", this.ThreadId) + `,`,
		`ReferencedTensor:` + strings.Replace(fmt.Sprintf("%v", this.ReferencedTensor), "AllocationDescription", "AllocationDescription", 1) + `,`,
		`MemoryStats:` + strings.Replace(fmt.Sprintf("%v", this.MemoryStats), "MemoryStats", "MemoryStats", 1) + `,`,
		`AllStartNanos:` + fmt.Sprintf("%v", this.AllStartNanos) + `,`,
		`OpStartRelNanos:` + fmt.Sprintf("%v", this.OpStartRelNanos) + `,`,
		`OpEndRelNanos:` + fmt.Sprintf("%v", this.OpEndRelNanos) + `,`,
		`AllEndRelNanos:` + fmt.Sprintf("%v", this.AllEndRelNanos) + `,`,
		`ScheduledNanos:` + fmt.Sprintf("%v", this.ScheduledNanos) + `,`,
		`}`,
	}, "")
	return s
}
func (this *DeviceStepStats) String() string {
	if this == nil {
		return "nil"
	}
	s := strings.Join([]string{`&DeviceStepStats{`,
		`Device:` + fmt.Sprintf("%v", this.Device) + `,`,
		`NodeStats:` + strings.Replace(fmt.Sprintf("%v", this.NodeStats), "NodeExecStats", "NodeExecStats", 1) + `,`,
		`}`,
	}, "")
	return s
}
func (this *StepStats) String() string {
	if this == nil {
		return "nil"
	}
	s := strings.Join([]string{`&StepStats{`,
		`DevStats:` + strings.Replace(fmt.Sprintf("%v", this.DevStats), "DeviceStepStats", "DeviceStepStats", 1) + `,`,
		`}`,
	}, "")
	return s
}
func valueToStringStepStats(v interface{}) string {
	rv := reflect.ValueOf(v)
	if rv.IsNil() {
		return "nil"
	}
	pv := reflect.Indirect(rv).Interface()
	return fmt.Sprintf("*%v", pv)
}
func (m *AllocationRecord) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowStepStats
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= (uint64(b) & 0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: AllocationRecord: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: AllocationRecord: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field AllocMicros", wireType)
			}
			m.AllocMicros = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.AllocMicros |= (int64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 2:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field AllocBytes", wireType)
			}
			m.AllocBytes = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.AllocBytes |= (int64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		default:
			iNdEx = preIndex
			skippy, err := skipStepStats(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthStepStats
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
func (m *AllocatorMemoryUsed) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowStepStats
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= (uint64(b) & 0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: AllocatorMemoryUsed: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: AllocatorMemoryUsed: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field AllocatorName", wireType)
			}
			var stringLen uint64
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				stringLen |= (uint64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			intStringLen := int(stringLen)
			if intStringLen < 0 {
				return ErrInvalidLengthStepStats
			}
			postIndex := iNdEx + intStringLen
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.AllocatorName = string(dAtA[iNdEx:postIndex])
			iNdEx = postIndex
		case 2:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field TotalBytes", wireType)
			}
			m.TotalBytes = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.TotalBytes |= (int64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 3:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field PeakBytes", wireType)
			}
			m.PeakBytes = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.PeakBytes |= (int64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 4:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field LiveBytes", wireType)
			}
			m.LiveBytes = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.LiveBytes |= (int64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 5:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field AllocatorBytesInUse", wireType)
			}
			m.AllocatorBytesInUse = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.AllocatorBytesInUse |= (int64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 6:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field AllocationRecords", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= (int(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthStepStats
			}
			postIndex := iNdEx + msglen
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.AllocationRecords = append(m.AllocationRecords, &AllocationRecord{})
			if err := m.AllocationRecords[len(m.AllocationRecords)-1].Unmarshal(dAtA[iNdEx:postIndex]); err != nil {
				return err
			}
			iNdEx = postIndex
		default:
			iNdEx = preIndex
			skippy, err := skipStepStats(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthStepStats
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
func (m *NodeOutput) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowStepStats
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= (uint64(b) & 0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: NodeOutput: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: NodeOutput: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field Slot", wireType)
			}
			m.Slot = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.Slot |= (int32(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 3:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field TensorDescription", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= (int(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthStepStats
			}
			postIndex := iNdEx + msglen
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			if m.TensorDescription == nil {
				m.TensorDescription = &TensorDescription{}
			}
			if err := m.TensorDescription.Unmarshal(dAtA[iNdEx:postIndex]); err != nil {
				return err
			}
			iNdEx = postIndex
		default:
			iNdEx = preIndex
			skippy, err := skipStepStats(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthStepStats
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
func (m *MemoryStats) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowStepStats
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= (uint64(b) & 0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: MemoryStats: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: MemoryStats: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field TempMemorySize", wireType)
			}
			m.TempMemorySize = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.TempMemorySize |= (int64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 2:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field DeviceTempMemorySize", wireType)
			}
			m.DeviceTempMemorySize = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.DeviceTempMemorySize |= (int64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 3:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field PersistentMemorySize", wireType)
			}
			m.PersistentMemorySize = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.PersistentMemorySize |= (int64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 4:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field DevicePersistentMemorySize", wireType)
			}
			m.DevicePersistentMemorySize = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.DevicePersistentMemorySize |= (int64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 5:
			if wireType == 0 {
				var v int64
				for shift := uint(0); ; shift += 7 {
					if shift >= 64 {
						return ErrIntOverflowStepStats
					}
					if iNdEx >= l {
						return io.ErrUnexpectedEOF
					}
					b := dAtA[iNdEx]
					iNdEx++
					v |= (int64(b) & 0x7F) << shift
					if b < 0x80 {
						break
					}
				}
				m.PersistentTensorAllocIds = append(m.PersistentTensorAllocIds, v)
			} else if wireType == 2 {
				var packedLen int
				for shift := uint(0); ; shift += 7 {
					if shift >= 64 {
						return ErrIntOverflowStepStats
					}
					if iNdEx >= l {
						return io.ErrUnexpectedEOF
					}
					b := dAtA[iNdEx]
					iNdEx++
					packedLen |= (int(b) & 0x7F) << shift
					if b < 0x80 {
						break
					}
				}
				if packedLen < 0 {
					return ErrInvalidLengthStepStats
				}
				postIndex := iNdEx + packedLen
				if postIndex > l {
					return io.ErrUnexpectedEOF
				}
				for iNdEx < postIndex {
					var v int64
					for shift := uint(0); ; shift += 7 {
						if shift >= 64 {
							return ErrIntOverflowStepStats
						}
						if iNdEx >= l {
							return io.ErrUnexpectedEOF
						}
						b := dAtA[iNdEx]
						iNdEx++
						v |= (int64(b) & 0x7F) << shift
						if b < 0x80 {
							break
						}
					}
					m.PersistentTensorAllocIds = append(m.PersistentTensorAllocIds, v)
				}
			} else {
				return fmt.Errorf("proto: wrong wireType = %d for field PersistentTensorAllocIds", wireType)
			}
		case 6:
			if wireType == 0 {
				var v int64
				for shift := uint(0); ; shift += 7 {
					if shift >= 64 {
						return ErrIntOverflowStepStats
					}
					if iNdEx >= l {
						return io.ErrUnexpectedEOF
					}
					b := dAtA[iNdEx]
					iNdEx++
					v |= (int64(b) & 0x7F) << shift
					if b < 0x80 {
						break
					}
				}
				m.DevicePersistentTensorAllocIds = append(m.DevicePersistentTensorAllocIds, v)
			} else if wireType == 2 {
				var packedLen int
				for shift := uint(0); ; shift += 7 {
					if shift >= 64 {
						return ErrIntOverflowStepStats
					}
					if iNdEx >= l {
						return io.ErrUnexpectedEOF
					}
					b := dAtA[iNdEx]
					iNdEx++
					packedLen |= (int(b) & 0x7F) << shift
					if b < 0x80 {
						break
					}
				}
				if packedLen < 0 {
					return ErrInvalidLengthStepStats
				}
				postIndex := iNdEx + packedLen
				if postIndex > l {
					return io.ErrUnexpectedEOF
				}
				for iNdEx < postIndex {
					var v int64
					for shift := uint(0); ; shift += 7 {
						if shift >= 64 {
							return ErrIntOverflowStepStats
						}
						if iNdEx >= l {
							return io.ErrUnexpectedEOF
						}
						b := dAtA[iNdEx]
						iNdEx++
						v |= (int64(b) & 0x7F) << shift
						if b < 0x80 {
							break
						}
					}
					m.DevicePersistentTensorAllocIds = append(m.DevicePersistentTensorAllocIds, v)
				}
			} else {
				return fmt.Errorf("proto: wrong wireType = %d for field DevicePersistentTensorAllocIds", wireType)
			}
		default:
			iNdEx = preIndex
			skippy, err := skipStepStats(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthStepStats
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
func (m *NodeExecStats) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowStepStats
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= (uint64(b) & 0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: NodeExecStats: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: NodeExecStats: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field NodeName", wireType)
			}
			var stringLen uint64
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				stringLen |= (uint64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			intStringLen := int(stringLen)
			if intStringLen < 0 {
				return ErrInvalidLengthStepStats
			}
			postIndex := iNdEx + intStringLen
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.NodeName = string(dAtA[iNdEx:postIndex])
			iNdEx = postIndex
		case 2:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field AllStartMicros", wireType)
			}
			m.AllStartMicros = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.AllStartMicros |= (int64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 3:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field OpStartRelMicros", wireType)
			}
			m.OpStartRelMicros = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.OpStartRelMicros |= (int64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 4:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field OpEndRelMicros", wireType)
			}
			m.OpEndRelMicros = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.OpEndRelMicros |= (int64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 5:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field AllEndRelMicros", wireType)
			}
			m.AllEndRelMicros = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.AllEndRelMicros |= (int64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 6:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field Memory", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= (int(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthStepStats
			}
			postIndex := iNdEx + msglen
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.Memory = append(m.Memory, &AllocatorMemoryUsed{})
			if err := m.Memory[len(m.Memory)-1].Unmarshal(dAtA[iNdEx:postIndex]); err != nil {
				return err
			}
			iNdEx = postIndex
		case 7:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field Output", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= (int(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthStepStats
			}
			postIndex := iNdEx + msglen
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.Output = append(m.Output, &NodeOutput{})
			if err := m.Output[len(m.Output)-1].Unmarshal(dAtA[iNdEx:postIndex]); err != nil {
				return err
			}
			iNdEx = postIndex
		case 8:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field TimelineLabel", wireType)
			}
			var stringLen uint64
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				stringLen |= (uint64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			intStringLen := int(stringLen)
			if intStringLen < 0 {
				return ErrInvalidLengthStepStats
			}
			postIndex := iNdEx + intStringLen
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.TimelineLabel = string(dAtA[iNdEx:postIndex])
			iNdEx = postIndex
		case 9:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field ScheduledMicros", wireType)
			}
			m.ScheduledMicros = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.ScheduledMicros |= (int64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 10:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field ThreadId", wireType)
			}
			m.ThreadId = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.ThreadId |= (uint32(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 11:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field ReferencedTensor", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= (int(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthStepStats
			}
			postIndex := iNdEx + msglen
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.ReferencedTensor = append(m.ReferencedTensor, &AllocationDescription{})
			if err := m.ReferencedTensor[len(m.ReferencedTensor)-1].Unmarshal(dAtA[iNdEx:postIndex]); err != nil {
				return err
			}
			iNdEx = postIndex
		case 12:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field MemoryStats", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= (int(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthStepStats
			}
			postIndex := iNdEx + msglen
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			if m.MemoryStats == nil {
				m.MemoryStats = &MemoryStats{}
			}
			if err := m.MemoryStats.Unmarshal(dAtA[iNdEx:postIndex]); err != nil {
				return err
			}
			iNdEx = postIndex
		case 13:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field AllStartNanos", wireType)
			}
			m.AllStartNanos = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.AllStartNanos |= (int64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 14:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field OpStartRelNanos", wireType)
			}
			m.OpStartRelNanos = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.OpStartRelNanos |= (int64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 15:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field OpEndRelNanos", wireType)
			}
			m.OpEndRelNanos = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.OpEndRelNanos |= (int64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 16:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field AllEndRelNanos", wireType)
			}
			m.AllEndRelNanos = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.AllEndRelNanos |= (int64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 17:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field ScheduledNanos", wireType)
			}
			m.ScheduledNanos = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.ScheduledNanos |= (int64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		default:
			iNdEx = preIndex
			skippy, err := skipStepStats(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthStepStats
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
func (m *DeviceStepStats) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowStepStats
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= (uint64(b) & 0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: DeviceStepStats: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: DeviceStepStats: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field Device", wireType)
			}
			var stringLen uint64
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				stringLen |= (uint64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			intStringLen := int(stringLen)
			if intStringLen < 0 {
				return ErrInvalidLengthStepStats
			}
			postIndex := iNdEx + intStringLen
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.Device = string(dAtA[iNdEx:postIndex])
			iNdEx = postIndex
		case 2:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field NodeStats", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= (int(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthStepStats
			}
			postIndex := iNdEx + msglen
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.NodeStats = append(m.NodeStats, &NodeExecStats{})
			if err := m.NodeStats[len(m.NodeStats)-1].Unmarshal(dAtA[iNdEx:postIndex]); err != nil {
				return err
			}
			iNdEx = postIndex
		default:
			iNdEx = preIndex
			skippy, err := skipStepStats(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthStepStats
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
func (m *StepStats) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowStepStats
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= (uint64(b) & 0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: StepStats: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: StepStats: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field DevStats", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= (int(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthStepStats
			}
			postIndex := iNdEx + msglen
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.DevStats = append(m.DevStats, &DeviceStepStats{})
			if err := m.DevStats[len(m.DevStats)-1].Unmarshal(dAtA[iNdEx:postIndex]); err != nil {
				return err
			}
			iNdEx = postIndex
		default:
			iNdEx = preIndex
			skippy, err := skipStepStats(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthStepStats
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
func skipStepStats(dAtA []byte) (n int, err error) {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return 0, ErrIntOverflowStepStats
			}
			if iNdEx >= l {
				return 0, io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= (uint64(b) & 0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		wireType := int(wire & 0x7)
		switch wireType {
		case 0:
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return 0, ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return 0, io.ErrUnexpectedEOF
				}
				iNdEx++
				if dAtA[iNdEx-1] < 0x80 {
					break
				}
			}
			return iNdEx, nil
		case 1:
			iNdEx += 8
			return iNdEx, nil
		case 2:
			var length int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return 0, ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return 0, io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				length |= (int(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			iNdEx += length
			if length < 0 {
				return 0, ErrInvalidLengthStepStats
			}
			return iNdEx, nil
		case 3:
			for {
				var innerWire uint64
				var start int = iNdEx
				for shift := uint(0); ; shift += 7 {
					if shift >= 64 {
						return 0, ErrIntOverflowStepStats
					}
					if iNdEx >= l {
						return 0, io.ErrUnexpectedEOF
					}
					b := dAtA[iNdEx]
					iNdEx++
					innerWire |= (uint64(b) & 0x7F) << shift
					if b < 0x80 {
						break
					}
				}
				innerWireType := int(innerWire & 0x7)
				if innerWireType == 4 {
					break
				}
				next, err := skipStepStats(dAtA[start:])
				if err != nil {
					return 0, err
				}
				iNdEx = start + next
			}
			return iNdEx, nil
		case 4:
			return iNdEx, nil
		case 5:
			iNdEx += 4
			return iNdEx, nil
		default:
			return 0, fmt.Errorf("proto: illegal wireType %d", wireType)
		}
	}
	panic("unreachable")
}

var (
	ErrInvalidLengthStepStats = fmt.Errorf("proto: negative length found during unmarshaling")
	ErrIntOverflowStepStats   = fmt.Errorf("proto: integer overflow")
)

func init() { proto.RegisterFile("step_stats.proto", fileDescriptor_step_stats_fae30a98b1fe86d1) }

var fileDescriptor_step_stats_fae30a98b1fe86d1 = []byte{
	// 903 bytes of a gzipped FileDescriptorProto
	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0xff, 0x74, 0x55, 0xc1, 0x72, 0x1b, 0x45,
	0x10, 0xcd, 0x4a, 0xb6, 0xb0, 0x5a, 0x96, 0xbd, 0x1a, 0x07, 0xb3, 0x65, 0x27, 0x8b, 0xd8, 0x2a,
	0x88, 0x5c, 0x10, 0xa7, 0x2a, 0xa1, 0x48, 0xe5, 0xc0, 0x01, 0x97, 0x7d, 0x30, 0x60, 0x43, 0xad,
	0x13, 0xae, 0x5b, 0x2b, 0x4d, 0x3b, 0xd9, 0xca, 0xee, 0xce, 0xd6, 0xce, 0xc8, 0x90, 0x9c, 0xf8,
	0x03, 0xf8, 0x00, 0x3e, 0x80, 0x4f, 0xe0, 0x13, 0x38, 0x72, 0xe4, 0x18, 0x89, 0x1f, 0xe0, 0xc8,
	0x89, 0xa2, 0xa6, 0x67, 0xb4, 0xbb, 0x92, 0xc2, 0x6d, 0xf6, 0xf5, 0xeb, 0x56, 0x4f, 0xf7, 0x7b,
	0x23, 0x70, 0xa5, 0xc2, 0x22, 0x92, 0x2a, 0x56, 0xf2, 0xb8, 0x28, 0x85, 0x12, 0xac, 0x7b, 0x5d,
	0xc6, 0x19, 0x7e, 0x2f, 0xca, 0x97, 0x07, 0xc1, 0x73, 0xf1, 0x5c, 0x3c, 0x20, 0x78, 0x3c, 0xbd,
	0x7e, 0xa0, 0xbf, 0xe8, 0x83, 0x4e, 0x86, 0x7e, 0x70, 0x27, 0x4e, 0x53, 0x31, 0x89, 0x55, 0x22,
	0xf2, 0x88, 0xa3, 0x9c, 0x94, 0x49, 0xa1, 0xcf, 0x36, 0xea, 0x29, 0xcc, 0xa5, 0x28, 0xd7, 0x23,
	0xc1, 0x77, 0xe0, 0x7e, 0x51, 0x65, 0x86, 0x38, 0x11, 0x25, 0x67, 0x1f, 0xc0, 0x36, 0x55, 0x8b,
	0xb2, 0x64, 0x52, 0x0a, 0xe9, 0x39, 0x43, 0x67, 0xd4, 0x0e, 0x7b, 0x84, 0x5d, 0x10, 0xc4, 0xde,
	0x07, 0xf3, 0x19, 0x8d, 0x5f, 0x29, 0x94, 0x5e, 0x8b, 0x18, 0x40, 0xd0, 0x89, 0x46, 0x82, 0x5f,
	0x5a, 0xb0, 0x67, 0x0b, 0x8b, 0xf2, 0x02, 0x33, 0x51, 0xbe, 0x7a, 0x26, 0x91, 0xb3, 0x0f, 0x61,
	0x27, 0x5e, 0xc0, 0x51, 0x1e, 0x67, 0x48, 0xd5, 0xbb, 0x61, 0xbf, 0x42, 0x2f, 0xe3, 0x0c, 0x75,
	0x7d, 0x25, 0x54, 0x9c, 0x2e, 0xd7, 0x27, 0x88, 0xea, 0xb3, 0xbb, 0x00, 0x05, 0xc6, 0x2f, 0x6d,
	0xbc, 0x4d, 0xf1, 0xae, 0x46, 0xaa, 0x70, 0x9a, 0xdc, 0xa0, 0x0d, 0x6f, 0x98, 0xb0, 0x46, 0x4c,
	0xf8, 0x11, 0xec, 0xd7, 0x5d, 0x10, 0x27, 0x4a, 0xf2, 0x68, 0x2a, 0xd1, 0xdb, 0x24, 0xea, 0x5e,
	0x15, 0x25, 0xfe, 0x79, 0xfe, 0x4c, 0x22, 0xfb, 0x12, 0x58, 0x63, 0xc8, 0x25, 0xcd, 0x4a, 0x7a,
	0x9d, 0x61, 0x7b, 0xd4, 0x7b, 0x78, 0x78, 0x5c, 0xad, 0xeb, 0x78, 0x75, 0x9e, 0xe1, 0x20, 0x5e,
	0x41, 0x64, 0x90, 0x01, 0x5c, 0x0a, 0x8e, 0xdf, 0x4c, 0x55, 0x31, 0x55, 0x8c, 0xc1, 0x86, 0x4c,
	0x85, 0xa2, 0x51, 0x6c, 0x86, 0x74, 0x66, 0x5f, 0x01, 0x5b, 0x5f, 0x1a, 0x5d, 0xb4, 0xf7, 0xf0,
	0x4e, 0xe3, 0xd7, 0x9e, 0x12, 0xe9, 0xb4, 0xe6, 0x84, 0x03, 0xb5, 0x0a, 0x05, 0xff, 0xb6, 0xa0,
	0x67, 0x96, 0x70, 0xa5, 0x25, 0xc6, 0x46, 0xe0, 0x2a, 0xcc, 0x8a, 0x28, 0x23, 0x2c, 0x92, 0xc9,
	0x6b, 0xb4, 0x5b, 0xde, 0xd1, 0xb8, 0xa5, 0x26, 0xaf, 0x91, 0x3d, 0x81, 0xf7, 0x38, 0xde, 0x24,
	0x13, 0x8c, 0xd6, 0x12, 0x68, 0x29, 0x27, 0x2d, 0xcf, 0x09, 0x6f, 0x1b, 0xca, 0xd3, 0xe5, 0xd4,
	0x4f, 0x61, 0xbf, 0xc0, 0x52, 0x26, 0x52, 0x61, 0xae, 0x96, 0x32, 0xcd, 0xba, 0x6e, 0xd7, 0xd1,
	0x46, 0xd6, 0x19, 0xdc, 0xb5, 0x3f, 0xf8, 0x3f, 0xc9, 0x1b, 0xd5, 0xcf, 0x1e, 0x18, 0xe2, 0xb7,
	0x6f, 0x2b, 0xf3, 0x39, 0x1c, 0x36, 0xf2, 0xed, 0x24, 0x8d, 0x64, 0x13, 0x2e, 0xbd, 0xcd, 0x61,
	0x7b, 0xd4, 0x0e, 0xbd, 0x9a, 0x62, 0xc6, 0x48, 0xab, 0x3b, 0xe7, 0x92, 0x5d, 0x42, 0xb0, 0xde,
	0xc5, 0x5a, 0x15, 0xbd, 0x7b, 0xd3, 0x8a, 0xbf, 0xda, 0xca, 0x72, 0xbd, 0xe0, 0xa7, 0x0e, 0xf4,
	0xf5, 0xc2, 0xcf, 0x7e, 0xc0, 0x89, 0x59, 0xc1, 0x21, 0x74, 0x73, 0xc1, 0xb1, 0xe9, 0x81, 0x2d,
	0x0d, 0x90, 0xfc, 0x47, 0xe0, 0xc6, 0x69, 0xaa, 0xdf, 0x83, 0x52, 0x2d, 0x5c, 0x68, 0x3c, 0xa0,
	0xdd, 0x73, 0xa5, 0x61, 0x6b, 0xc4, 0xfb, 0xb0, 0x27, 0x0a, 0x4b, 0x2c, 0x31, 0x5d, 0x90, 0xcd,
	0x84, 0x5d, 0x51, 0x10, 0x37, 0xc4, 0xd4, 0xd2, 0x8f, 0x60, 0x20, 0x8a, 0x08, 0x73, 0xde, 0x24,
	0x1b, 0x7b, 0xec, 0x88, 0xe2, 0x2c, 0xe7, 0x35, 0xf5, 0x63, 0x92, 0xfb, 0x2a, 0xd7, 0xf8, 0x63,
	0x37, 0x4e, 0xd3, 0x25, 0xf2, 0x67, 0xd0, 0x31, 0x3b, 0xb2, 0x7e, 0xf0, 0xd7, 0xfd, 0xd0, 0x7c,
	0x06, 0x42, 0xcb, 0x66, 0xf7, 0xa1, 0x23, 0xc8, 0x03, 0xde, 0x3b, 0x94, 0xf7, 0x6e, 0x23, 0xaf,
	0x36, 0x48, 0x68, 0x49, 0xfa, 0xf5, 0x50, 0x49, 0x86, 0x69, 0x92, 0x63, 0x94, 0xc6, 0x63, 0x4c,
	0xbd, 0x2d, 0xf3, 0x7a, 0x2c, 0xd0, 0xaf, 0x35, 0xc8, 0x8e, 0xc0, 0x95, 0x93, 0x17, 0xc8, 0xa7,
	0x29, 0xf2, 0x45, 0xe3, 0x5d, 0xd3, 0x78, 0x85, 0xdb, 0xc6, 0x0f, 0xa1, 0xab, 0x5e, 0x94, 0x18,
	0xf3, 0x28, 0xe1, 0x1e, 0x0c, 0x9d, 0x51, 0x3f, 0xdc, 0x32, 0xc0, 0x39, 0x67, 0x17, 0x30, 0x28,
	0xf1, 0x1a, 0x4b, 0xcc, 0x27, 0xc8, 0xed, 0xfa, 0xbd, 0x1e, 0x35, 0x3a, 0x7c, 0xab, 0xe1, 0x9b,
	0x36, 0x74, 0xeb, 0x54, 0x23, 0x06, 0xf6, 0x04, 0xb6, 0x17, 0x42, 0xd6, 0x12, 0xf0, 0xb6, 0xc9,
	0xcc, 0xfb, 0x8d, 0x4a, 0x0d, 0x8f, 0x86, 0xbd, 0xac, 0x61, 0xd8, 0x8f, 0x60, 0xb7, 0x16, 0x44,
	0x1e, 0xe7, 0x42, 0x7a, 0x7d, 0xba, 0x50, 0x7f, 0xa1, 0x87, 0x4b, 0x0d, 0xea, 0xa5, 0x2d, 0xc9,
	0xc1, 0x50, 0x77, 0xcc, 0xdd, 0x6b, 0x35, 0x18, 0xf2, 0x3d, 0x70, 0x1b, 0x62, 0x30, 0xd4, 0x5d,
	0x53, 0x75, 0xa1, 0x05, 0x43, 0x3c, 0x82, 0x41, 0x53, 0x0a, 0x86, 0xe9, 0x56, 0x7a, 0x6c, 0x52,
	0xef, 0x41, 0x3d, 0x62, 0x4b, 0x1c, 0x18, 0x62, 0x05, 0x13, 0x31, 0x18, 0xc3, 0xee, 0x29, 0x79,
	0xe6, 0x4a, 0x61, 0x61, 0x2e, 0xb9, 0x0f, 0x1d, 0x63, 0x23, 0xeb, 0x07, 0xfb, 0xc5, 0x1e, 0x03,
	0x90, 0x55, 0xcc, 0xd4, 0x5a, 0x34, 0x7f, 0x6f, 0x45, 0x28, 0x95, 0xb1, 0x42, 0xb2, 0x15, 0x1d,
	0x83, 0x53, 0xe8, 0xd6, 0xd5, 0x1f, 0x43, 0x97, 0xe3, 0x8d, 0x2d, 0xe2, 0x50, 0x91, 0x83, 0x46,
	0x91, 0x95, 0x66, 0xc2, 0x2d, 0x8e, 0x37, 0x74, 0x3a, 0xf9, 0xe4, 0xcf, 0x99, 0x7f, 0xeb, 0xcd,
	0xcc, 0x77, 0xfe, 0x9e, 0xf9, 0xce, 0x3f, 0x33, 0xdf, 0xf9, 0x71, 0xee, 0x3b, 0xbf, 0xce, 0x7d,
	0xe7, 0xb7, 0xb9, 0xef, 0xfc, 0x3e, 0xf7, 0x9d, 0x3f, 0xe6, 0xbe, 0xf3, 0x66, 0xee, 0x3b, 0x3f,
	0xff, 0xe5, 0xdf, 0x1a, 0x77, 0xe8, 0x7f, 0xf5, 0xd1, 0x7f, 0x01, 0x00, 0x00, 0xff, 0xff, 0x76,
	0x93, 0x7c, 0xf9, 0xd2, 0x07, 0x00, 0x00,
}
