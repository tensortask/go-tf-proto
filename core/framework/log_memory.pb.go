// Code generated by protoc-gen-gogo. DO NOT EDIT.
// source: log_memory.proto

package framework

import proto "github.com/gogo/protobuf/proto"
import fmt "fmt"
import math "math"
import _ "github.com/gogo/protobuf/gogoproto"

import strings "strings"
import reflect "reflect"

import io "io"

// Reference imports to suppress errors if they are not otherwise used.
var _ = proto.Marshal
var _ = fmt.Errorf
var _ = math.Inf

// This is a compile-time assertion to ensure that this generated file
// is compatible with the proto package it is being compiled against.
// A compilation error at this line likely means your copy of the
// proto package needs to be updated.
const _ = proto.GoGoProtoPackageIsVersion2 // please upgrade the proto package

type MemoryLogStep struct {
	// Process-unique step id.
	StepId int64 `protobuf:"varint,1,opt,name=step_id,json=stepId,proto3" json:"step_id,omitempty"`
	// Handle describing the feeds and fetches of the step.
	Handle               string   `protobuf:"bytes,2,opt,name=handle,proto3" json:"handle,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *MemoryLogStep) Reset()      { *m = MemoryLogStep{} }
func (*MemoryLogStep) ProtoMessage() {}
func (*MemoryLogStep) Descriptor() ([]byte, []int) {
	return fileDescriptor_log_memory_169de76e43ea0cf0, []int{0}
}
func (m *MemoryLogStep) XXX_Unmarshal(b []byte) error {
	return m.Unmarshal(b)
}
func (m *MemoryLogStep) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	if deterministic {
		return xxx_messageInfo_MemoryLogStep.Marshal(b, m, deterministic)
	} else {
		b = b[:cap(b)]
		n, err := m.MarshalTo(b)
		if err != nil {
			return nil, err
		}
		return b[:n], nil
	}
}
func (dst *MemoryLogStep) XXX_Merge(src proto.Message) {
	xxx_messageInfo_MemoryLogStep.Merge(dst, src)
}
func (m *MemoryLogStep) XXX_Size() int {
	return m.Size()
}
func (m *MemoryLogStep) XXX_DiscardUnknown() {
	xxx_messageInfo_MemoryLogStep.DiscardUnknown(m)
}

var xxx_messageInfo_MemoryLogStep proto.InternalMessageInfo

func (m *MemoryLogStep) GetStepId() int64 {
	if m != nil {
		return m.StepId
	}
	return 0
}

func (m *MemoryLogStep) GetHandle() string {
	if m != nil {
		return m.Handle
	}
	return ""
}

type MemoryLogTensorAllocation struct {
	// Process-unique step id.
	StepId int64 `protobuf:"varint,1,opt,name=step_id,json=stepId,proto3" json:"step_id,omitempty"`
	// Name of the kernel making the allocation as set in GraphDef,
	// e.g., "affine2/weights/Assign".
	KernelName string `protobuf:"bytes,2,opt,name=kernel_name,json=kernelName,proto3" json:"kernel_name,omitempty"`
	// Allocated tensor details.
	Tensor               *TensorDescription `protobuf:"bytes,3,opt,name=tensor" json:"tensor,omitempty"`
	XXX_NoUnkeyedLiteral struct{}           `json:"-"`
	XXX_sizecache        int32              `json:"-"`
}

func (m *MemoryLogTensorAllocation) Reset()      { *m = MemoryLogTensorAllocation{} }
func (*MemoryLogTensorAllocation) ProtoMessage() {}
func (*MemoryLogTensorAllocation) Descriptor() ([]byte, []int) {
	return fileDescriptor_log_memory_169de76e43ea0cf0, []int{1}
}
func (m *MemoryLogTensorAllocation) XXX_Unmarshal(b []byte) error {
	return m.Unmarshal(b)
}
func (m *MemoryLogTensorAllocation) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	if deterministic {
		return xxx_messageInfo_MemoryLogTensorAllocation.Marshal(b, m, deterministic)
	} else {
		b = b[:cap(b)]
		n, err := m.MarshalTo(b)
		if err != nil {
			return nil, err
		}
		return b[:n], nil
	}
}
func (dst *MemoryLogTensorAllocation) XXX_Merge(src proto.Message) {
	xxx_messageInfo_MemoryLogTensorAllocation.Merge(dst, src)
}
func (m *MemoryLogTensorAllocation) XXX_Size() int {
	return m.Size()
}
func (m *MemoryLogTensorAllocation) XXX_DiscardUnknown() {
	xxx_messageInfo_MemoryLogTensorAllocation.DiscardUnknown(m)
}

var xxx_messageInfo_MemoryLogTensorAllocation proto.InternalMessageInfo

func (m *MemoryLogTensorAllocation) GetStepId() int64 {
	if m != nil {
		return m.StepId
	}
	return 0
}

func (m *MemoryLogTensorAllocation) GetKernelName() string {
	if m != nil {
		return m.KernelName
	}
	return ""
}

func (m *MemoryLogTensorAllocation) GetTensor() *TensorDescription {
	if m != nil {
		return m.Tensor
	}
	return nil
}

type MemoryLogTensorDeallocation struct {
	// Id of the tensor buffer being deallocated, used to match to a
	// corresponding allocation.
	AllocationId int64 `protobuf:"varint,1,opt,name=allocation_id,json=allocationId,proto3" json:"allocation_id,omitempty"`
	// Name of the allocator used.
	AllocatorName        string   `protobuf:"bytes,2,opt,name=allocator_name,json=allocatorName,proto3" json:"allocator_name,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *MemoryLogTensorDeallocation) Reset()      { *m = MemoryLogTensorDeallocation{} }
func (*MemoryLogTensorDeallocation) ProtoMessage() {}
func (*MemoryLogTensorDeallocation) Descriptor() ([]byte, []int) {
	return fileDescriptor_log_memory_169de76e43ea0cf0, []int{2}
}
func (m *MemoryLogTensorDeallocation) XXX_Unmarshal(b []byte) error {
	return m.Unmarshal(b)
}
func (m *MemoryLogTensorDeallocation) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	if deterministic {
		return xxx_messageInfo_MemoryLogTensorDeallocation.Marshal(b, m, deterministic)
	} else {
		b = b[:cap(b)]
		n, err := m.MarshalTo(b)
		if err != nil {
			return nil, err
		}
		return b[:n], nil
	}
}
func (dst *MemoryLogTensorDeallocation) XXX_Merge(src proto.Message) {
	xxx_messageInfo_MemoryLogTensorDeallocation.Merge(dst, src)
}
func (m *MemoryLogTensorDeallocation) XXX_Size() int {
	return m.Size()
}
func (m *MemoryLogTensorDeallocation) XXX_DiscardUnknown() {
	xxx_messageInfo_MemoryLogTensorDeallocation.DiscardUnknown(m)
}

var xxx_messageInfo_MemoryLogTensorDeallocation proto.InternalMessageInfo

func (m *MemoryLogTensorDeallocation) GetAllocationId() int64 {
	if m != nil {
		return m.AllocationId
	}
	return 0
}

func (m *MemoryLogTensorDeallocation) GetAllocatorName() string {
	if m != nil {
		return m.AllocatorName
	}
	return ""
}

type MemoryLogTensorOutput struct {
	// Process-unique step id.
	StepId int64 `protobuf:"varint,1,opt,name=step_id,json=stepId,proto3" json:"step_id,omitempty"`
	// Name of the kernel producing an output as set in GraphDef, e.g.,
	// "affine2/weights/Assign".
	KernelName string `protobuf:"bytes,2,opt,name=kernel_name,json=kernelName,proto3" json:"kernel_name,omitempty"`
	// Index of the output being set.
	Index int32 `protobuf:"varint,3,opt,name=index,proto3" json:"index,omitempty"`
	// Output tensor details.
	Tensor               *TensorDescription `protobuf:"bytes,4,opt,name=tensor" json:"tensor,omitempty"`
	XXX_NoUnkeyedLiteral struct{}           `json:"-"`
	XXX_sizecache        int32              `json:"-"`
}

func (m *MemoryLogTensorOutput) Reset()      { *m = MemoryLogTensorOutput{} }
func (*MemoryLogTensorOutput) ProtoMessage() {}
func (*MemoryLogTensorOutput) Descriptor() ([]byte, []int) {
	return fileDescriptor_log_memory_169de76e43ea0cf0, []int{3}
}
func (m *MemoryLogTensorOutput) XXX_Unmarshal(b []byte) error {
	return m.Unmarshal(b)
}
func (m *MemoryLogTensorOutput) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	if deterministic {
		return xxx_messageInfo_MemoryLogTensorOutput.Marshal(b, m, deterministic)
	} else {
		b = b[:cap(b)]
		n, err := m.MarshalTo(b)
		if err != nil {
			return nil, err
		}
		return b[:n], nil
	}
}
func (dst *MemoryLogTensorOutput) XXX_Merge(src proto.Message) {
	xxx_messageInfo_MemoryLogTensorOutput.Merge(dst, src)
}
func (m *MemoryLogTensorOutput) XXX_Size() int {
	return m.Size()
}
func (m *MemoryLogTensorOutput) XXX_DiscardUnknown() {
	xxx_messageInfo_MemoryLogTensorOutput.DiscardUnknown(m)
}

var xxx_messageInfo_MemoryLogTensorOutput proto.InternalMessageInfo

func (m *MemoryLogTensorOutput) GetStepId() int64 {
	if m != nil {
		return m.StepId
	}
	return 0
}

func (m *MemoryLogTensorOutput) GetKernelName() string {
	if m != nil {
		return m.KernelName
	}
	return ""
}

func (m *MemoryLogTensorOutput) GetIndex() int32 {
	if m != nil {
		return m.Index
	}
	return 0
}

func (m *MemoryLogTensorOutput) GetTensor() *TensorDescription {
	if m != nil {
		return m.Tensor
	}
	return nil
}

type MemoryLogRawAllocation struct {
	// Process-unique step id.
	StepId int64 `protobuf:"varint,1,opt,name=step_id,json=stepId,proto3" json:"step_id,omitempty"`
	// Name of the operation making the allocation.
	Operation string `protobuf:"bytes,2,opt,name=operation,proto3" json:"operation,omitempty"`
	// Number of bytes in the allocation.
	NumBytes int64 `protobuf:"varint,3,opt,name=num_bytes,json=numBytes,proto3" json:"num_bytes,omitempty"`
	// Address of the allocation.
	Ptr uint64 `protobuf:"varint,4,opt,name=ptr,proto3" json:"ptr,omitempty"`
	// Id of the tensor buffer being allocated, used to match to a
	// corresponding deallocation.
	AllocationId int64 `protobuf:"varint,5,opt,name=allocation_id,json=allocationId,proto3" json:"allocation_id,omitempty"`
	// Name of the allocator used.
	AllocatorName        string   `protobuf:"bytes,6,opt,name=allocator_name,json=allocatorName,proto3" json:"allocator_name,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *MemoryLogRawAllocation) Reset()      { *m = MemoryLogRawAllocation{} }
func (*MemoryLogRawAllocation) ProtoMessage() {}
func (*MemoryLogRawAllocation) Descriptor() ([]byte, []int) {
	return fileDescriptor_log_memory_169de76e43ea0cf0, []int{4}
}
func (m *MemoryLogRawAllocation) XXX_Unmarshal(b []byte) error {
	return m.Unmarshal(b)
}
func (m *MemoryLogRawAllocation) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	if deterministic {
		return xxx_messageInfo_MemoryLogRawAllocation.Marshal(b, m, deterministic)
	} else {
		b = b[:cap(b)]
		n, err := m.MarshalTo(b)
		if err != nil {
			return nil, err
		}
		return b[:n], nil
	}
}
func (dst *MemoryLogRawAllocation) XXX_Merge(src proto.Message) {
	xxx_messageInfo_MemoryLogRawAllocation.Merge(dst, src)
}
func (m *MemoryLogRawAllocation) XXX_Size() int {
	return m.Size()
}
func (m *MemoryLogRawAllocation) XXX_DiscardUnknown() {
	xxx_messageInfo_MemoryLogRawAllocation.DiscardUnknown(m)
}

var xxx_messageInfo_MemoryLogRawAllocation proto.InternalMessageInfo

func (m *MemoryLogRawAllocation) GetStepId() int64 {
	if m != nil {
		return m.StepId
	}
	return 0
}

func (m *MemoryLogRawAllocation) GetOperation() string {
	if m != nil {
		return m.Operation
	}
	return ""
}

func (m *MemoryLogRawAllocation) GetNumBytes() int64 {
	if m != nil {
		return m.NumBytes
	}
	return 0
}

func (m *MemoryLogRawAllocation) GetPtr() uint64 {
	if m != nil {
		return m.Ptr
	}
	return 0
}

func (m *MemoryLogRawAllocation) GetAllocationId() int64 {
	if m != nil {
		return m.AllocationId
	}
	return 0
}

func (m *MemoryLogRawAllocation) GetAllocatorName() string {
	if m != nil {
		return m.AllocatorName
	}
	return ""
}

type MemoryLogRawDeallocation struct {
	// Process-unique step id.
	StepId int64 `protobuf:"varint,1,opt,name=step_id,json=stepId,proto3" json:"step_id,omitempty"`
	// Name of the operation making the deallocation.
	Operation string `protobuf:"bytes,2,opt,name=operation,proto3" json:"operation,omitempty"`
	// Id of the tensor buffer being deallocated, used to match to a
	// corresponding allocation.
	AllocationId int64 `protobuf:"varint,3,opt,name=allocation_id,json=allocationId,proto3" json:"allocation_id,omitempty"`
	// Name of the allocator used.
	AllocatorName string `protobuf:"bytes,4,opt,name=allocator_name,json=allocatorName,proto3" json:"allocator_name,omitempty"`
	// True if the deallocation is queued and will be performed later,
	// e.g. for GPU lazy freeing of buffers.
	Deferred             bool     `protobuf:"varint,5,opt,name=deferred,proto3" json:"deferred,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *MemoryLogRawDeallocation) Reset()      { *m = MemoryLogRawDeallocation{} }
func (*MemoryLogRawDeallocation) ProtoMessage() {}
func (*MemoryLogRawDeallocation) Descriptor() ([]byte, []int) {
	return fileDescriptor_log_memory_169de76e43ea0cf0, []int{5}
}
func (m *MemoryLogRawDeallocation) XXX_Unmarshal(b []byte) error {
	return m.Unmarshal(b)
}
func (m *MemoryLogRawDeallocation) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	if deterministic {
		return xxx_messageInfo_MemoryLogRawDeallocation.Marshal(b, m, deterministic)
	} else {
		b = b[:cap(b)]
		n, err := m.MarshalTo(b)
		if err != nil {
			return nil, err
		}
		return b[:n], nil
	}
}
func (dst *MemoryLogRawDeallocation) XXX_Merge(src proto.Message) {
	xxx_messageInfo_MemoryLogRawDeallocation.Merge(dst, src)
}
func (m *MemoryLogRawDeallocation) XXX_Size() int {
	return m.Size()
}
func (m *MemoryLogRawDeallocation) XXX_DiscardUnknown() {
	xxx_messageInfo_MemoryLogRawDeallocation.DiscardUnknown(m)
}

var xxx_messageInfo_MemoryLogRawDeallocation proto.InternalMessageInfo

func (m *MemoryLogRawDeallocation) GetStepId() int64 {
	if m != nil {
		return m.StepId
	}
	return 0
}

func (m *MemoryLogRawDeallocation) GetOperation() string {
	if m != nil {
		return m.Operation
	}
	return ""
}

func (m *MemoryLogRawDeallocation) GetAllocationId() int64 {
	if m != nil {
		return m.AllocationId
	}
	return 0
}

func (m *MemoryLogRawDeallocation) GetAllocatorName() string {
	if m != nil {
		return m.AllocatorName
	}
	return ""
}

func (m *MemoryLogRawDeallocation) GetDeferred() bool {
	if m != nil {
		return m.Deferred
	}
	return false
}

func init() {
	proto.RegisterType((*MemoryLogStep)(nil), "framework.MemoryLogStep")
	proto.RegisterType((*MemoryLogTensorAllocation)(nil), "framework.MemoryLogTensorAllocation")
	proto.RegisterType((*MemoryLogTensorDeallocation)(nil), "framework.MemoryLogTensorDeallocation")
	proto.RegisterType((*MemoryLogTensorOutput)(nil), "framework.MemoryLogTensorOutput")
	proto.RegisterType((*MemoryLogRawAllocation)(nil), "framework.MemoryLogRawAllocation")
	proto.RegisterType((*MemoryLogRawDeallocation)(nil), "framework.MemoryLogRawDeallocation")
}
func (this *MemoryLogStep) VerboseEqual(that interface{}) error {
	if that == nil {
		if this == nil {
			return nil
		}
		return fmt.Errorf("that == nil && this != nil")
	}

	that1, ok := that.(*MemoryLogStep)
	if !ok {
		that2, ok := that.(MemoryLogStep)
		if ok {
			that1 = &that2
		} else {
			return fmt.Errorf("that is not of type *MemoryLogStep")
		}
	}
	if that1 == nil {
		if this == nil {
			return nil
		}
		return fmt.Errorf("that is type *MemoryLogStep but is nil && this != nil")
	} else if this == nil {
		return fmt.Errorf("that is type *MemoryLogStep but is not nil && this == nil")
	}
	if this.StepId != that1.StepId {
		return fmt.Errorf("StepId this(%v) Not Equal that(%v)", this.StepId, that1.StepId)
	}
	if this.Handle != that1.Handle {
		return fmt.Errorf("Handle this(%v) Not Equal that(%v)", this.Handle, that1.Handle)
	}
	return nil
}
func (this *MemoryLogStep) Equal(that interface{}) bool {
	if that == nil {
		return this == nil
	}

	that1, ok := that.(*MemoryLogStep)
	if !ok {
		that2, ok := that.(MemoryLogStep)
		if ok {
			that1 = &that2
		} else {
			return false
		}
	}
	if that1 == nil {
		return this == nil
	} else if this == nil {
		return false
	}
	if this.StepId != that1.StepId {
		return false
	}
	if this.Handle != that1.Handle {
		return false
	}
	return true
}
func (this *MemoryLogTensorAllocation) VerboseEqual(that interface{}) error {
	if that == nil {
		if this == nil {
			return nil
		}
		return fmt.Errorf("that == nil && this != nil")
	}

	that1, ok := that.(*MemoryLogTensorAllocation)
	if !ok {
		that2, ok := that.(MemoryLogTensorAllocation)
		if ok {
			that1 = &that2
		} else {
			return fmt.Errorf("that is not of type *MemoryLogTensorAllocation")
		}
	}
	if that1 == nil {
		if this == nil {
			return nil
		}
		return fmt.Errorf("that is type *MemoryLogTensorAllocation but is nil && this != nil")
	} else if this == nil {
		return fmt.Errorf("that is type *MemoryLogTensorAllocation but is not nil && this == nil")
	}
	if this.StepId != that1.StepId {
		return fmt.Errorf("StepId this(%v) Not Equal that(%v)", this.StepId, that1.StepId)
	}
	if this.KernelName != that1.KernelName {
		return fmt.Errorf("KernelName this(%v) Not Equal that(%v)", this.KernelName, that1.KernelName)
	}
	if !this.Tensor.Equal(that1.Tensor) {
		return fmt.Errorf("Tensor this(%v) Not Equal that(%v)", this.Tensor, that1.Tensor)
	}
	return nil
}
func (this *MemoryLogTensorAllocation) Equal(that interface{}) bool {
	if that == nil {
		return this == nil
	}

	that1, ok := that.(*MemoryLogTensorAllocation)
	if !ok {
		that2, ok := that.(MemoryLogTensorAllocation)
		if ok {
			that1 = &that2
		} else {
			return false
		}
	}
	if that1 == nil {
		return this == nil
	} else if this == nil {
		return false
	}
	if this.StepId != that1.StepId {
		return false
	}
	if this.KernelName != that1.KernelName {
		return false
	}
	if !this.Tensor.Equal(that1.Tensor) {
		return false
	}
	return true
}
func (this *MemoryLogTensorDeallocation) VerboseEqual(that interface{}) error {
	if that == nil {
		if this == nil {
			return nil
		}
		return fmt.Errorf("that == nil && this != nil")
	}

	that1, ok := that.(*MemoryLogTensorDeallocation)
	if !ok {
		that2, ok := that.(MemoryLogTensorDeallocation)
		if ok {
			that1 = &that2
		} else {
			return fmt.Errorf("that is not of type *MemoryLogTensorDeallocation")
		}
	}
	if that1 == nil {
		if this == nil {
			return nil
		}
		return fmt.Errorf("that is type *MemoryLogTensorDeallocation but is nil && this != nil")
	} else if this == nil {
		return fmt.Errorf("that is type *MemoryLogTensorDeallocation but is not nil && this == nil")
	}
	if this.AllocationId != that1.AllocationId {
		return fmt.Errorf("AllocationId this(%v) Not Equal that(%v)", this.AllocationId, that1.AllocationId)
	}
	if this.AllocatorName != that1.AllocatorName {
		return fmt.Errorf("AllocatorName this(%v) Not Equal that(%v)", this.AllocatorName, that1.AllocatorName)
	}
	return nil
}
func (this *MemoryLogTensorDeallocation) Equal(that interface{}) bool {
	if that == nil {
		return this == nil
	}

	that1, ok := that.(*MemoryLogTensorDeallocation)
	if !ok {
		that2, ok := that.(MemoryLogTensorDeallocation)
		if ok {
			that1 = &that2
		} else {
			return false
		}
	}
	if that1 == nil {
		return this == nil
	} else if this == nil {
		return false
	}
	if this.AllocationId != that1.AllocationId {
		return false
	}
	if this.AllocatorName != that1.AllocatorName {
		return false
	}
	return true
}
func (this *MemoryLogTensorOutput) VerboseEqual(that interface{}) error {
	if that == nil {
		if this == nil {
			return nil
		}
		return fmt.Errorf("that == nil && this != nil")
	}

	that1, ok := that.(*MemoryLogTensorOutput)
	if !ok {
		that2, ok := that.(MemoryLogTensorOutput)
		if ok {
			that1 = &that2
		} else {
			return fmt.Errorf("that is not of type *MemoryLogTensorOutput")
		}
	}
	if that1 == nil {
		if this == nil {
			return nil
		}
		return fmt.Errorf("that is type *MemoryLogTensorOutput but is nil && this != nil")
	} else if this == nil {
		return fmt.Errorf("that is type *MemoryLogTensorOutput but is not nil && this == nil")
	}
	if this.StepId != that1.StepId {
		return fmt.Errorf("StepId this(%v) Not Equal that(%v)", this.StepId, that1.StepId)
	}
	if this.KernelName != that1.KernelName {
		return fmt.Errorf("KernelName this(%v) Not Equal that(%v)", this.KernelName, that1.KernelName)
	}
	if this.Index != that1.Index {
		return fmt.Errorf("Index this(%v) Not Equal that(%v)", this.Index, that1.Index)
	}
	if !this.Tensor.Equal(that1.Tensor) {
		return fmt.Errorf("Tensor this(%v) Not Equal that(%v)", this.Tensor, that1.Tensor)
	}
	return nil
}
func (this *MemoryLogTensorOutput) Equal(that interface{}) bool {
	if that == nil {
		return this == nil
	}

	that1, ok := that.(*MemoryLogTensorOutput)
	if !ok {
		that2, ok := that.(MemoryLogTensorOutput)
		if ok {
			that1 = &that2
		} else {
			return false
		}
	}
	if that1 == nil {
		return this == nil
	} else if this == nil {
		return false
	}
	if this.StepId != that1.StepId {
		return false
	}
	if this.KernelName != that1.KernelName {
		return false
	}
	if this.Index != that1.Index {
		return false
	}
	if !this.Tensor.Equal(that1.Tensor) {
		return false
	}
	return true
}
func (this *MemoryLogRawAllocation) VerboseEqual(that interface{}) error {
	if that == nil {
		if this == nil {
			return nil
		}
		return fmt.Errorf("that == nil && this != nil")
	}

	that1, ok := that.(*MemoryLogRawAllocation)
	if !ok {
		that2, ok := that.(MemoryLogRawAllocation)
		if ok {
			that1 = &that2
		} else {
			return fmt.Errorf("that is not of type *MemoryLogRawAllocation")
		}
	}
	if that1 == nil {
		if this == nil {
			return nil
		}
		return fmt.Errorf("that is type *MemoryLogRawAllocation but is nil && this != nil")
	} else if this == nil {
		return fmt.Errorf("that is type *MemoryLogRawAllocation but is not nil && this == nil")
	}
	if this.StepId != that1.StepId {
		return fmt.Errorf("StepId this(%v) Not Equal that(%v)", this.StepId, that1.StepId)
	}
	if this.Operation != that1.Operation {
		return fmt.Errorf("Operation this(%v) Not Equal that(%v)", this.Operation, that1.Operation)
	}
	if this.NumBytes != that1.NumBytes {
		return fmt.Errorf("NumBytes this(%v) Not Equal that(%v)", this.NumBytes, that1.NumBytes)
	}
	if this.Ptr != that1.Ptr {
		return fmt.Errorf("Ptr this(%v) Not Equal that(%v)", this.Ptr, that1.Ptr)
	}
	if this.AllocationId != that1.AllocationId {
		return fmt.Errorf("AllocationId this(%v) Not Equal that(%v)", this.AllocationId, that1.AllocationId)
	}
	if this.AllocatorName != that1.AllocatorName {
		return fmt.Errorf("AllocatorName this(%v) Not Equal that(%v)", this.AllocatorName, that1.AllocatorName)
	}
	return nil
}
func (this *MemoryLogRawAllocation) Equal(that interface{}) bool {
	if that == nil {
		return this == nil
	}

	that1, ok := that.(*MemoryLogRawAllocation)
	if !ok {
		that2, ok := that.(MemoryLogRawAllocation)
		if ok {
			that1 = &that2
		} else {
			return false
		}
	}
	if that1 == nil {
		return this == nil
	} else if this == nil {
		return false
	}
	if this.StepId != that1.StepId {
		return false
	}
	if this.Operation != that1.Operation {
		return false
	}
	if this.NumBytes != that1.NumBytes {
		return false
	}
	if this.Ptr != that1.Ptr {
		return false
	}
	if this.AllocationId != that1.AllocationId {
		return false
	}
	if this.AllocatorName != that1.AllocatorName {
		return false
	}
	return true
}
func (this *MemoryLogRawDeallocation) VerboseEqual(that interface{}) error {
	if that == nil {
		if this == nil {
			return nil
		}
		return fmt.Errorf("that == nil && this != nil")
	}

	that1, ok := that.(*MemoryLogRawDeallocation)
	if !ok {
		that2, ok := that.(MemoryLogRawDeallocation)
		if ok {
			that1 = &that2
		} else {
			return fmt.Errorf("that is not of type *MemoryLogRawDeallocation")
		}
	}
	if that1 == nil {
		if this == nil {
			return nil
		}
		return fmt.Errorf("that is type *MemoryLogRawDeallocation but is nil && this != nil")
	} else if this == nil {
		return fmt.Errorf("that is type *MemoryLogRawDeallocation but is not nil && this == nil")
	}
	if this.StepId != that1.StepId {
		return fmt.Errorf("StepId this(%v) Not Equal that(%v)", this.StepId, that1.StepId)
	}
	if this.Operation != that1.Operation {
		return fmt.Errorf("Operation this(%v) Not Equal that(%v)", this.Operation, that1.Operation)
	}
	if this.AllocationId != that1.AllocationId {
		return fmt.Errorf("AllocationId this(%v) Not Equal that(%v)", this.AllocationId, that1.AllocationId)
	}
	if this.AllocatorName != that1.AllocatorName {
		return fmt.Errorf("AllocatorName this(%v) Not Equal that(%v)", this.AllocatorName, that1.AllocatorName)
	}
	if this.Deferred != that1.Deferred {
		return fmt.Errorf("Deferred this(%v) Not Equal that(%v)", this.Deferred, that1.Deferred)
	}
	return nil
}
func (this *MemoryLogRawDeallocation) Equal(that interface{}) bool {
	if that == nil {
		return this == nil
	}

	that1, ok := that.(*MemoryLogRawDeallocation)
	if !ok {
		that2, ok := that.(MemoryLogRawDeallocation)
		if ok {
			that1 = &that2
		} else {
			return false
		}
	}
	if that1 == nil {
		return this == nil
	} else if this == nil {
		return false
	}
	if this.StepId != that1.StepId {
		return false
	}
	if this.Operation != that1.Operation {
		return false
	}
	if this.AllocationId != that1.AllocationId {
		return false
	}
	if this.AllocatorName != that1.AllocatorName {
		return false
	}
	if this.Deferred != that1.Deferred {
		return false
	}
	return true
}
func (this *MemoryLogStep) GoString() string {
	if this == nil {
		return "nil"
	}
	s := make([]string, 0, 6)
	s = append(s, "&framework.MemoryLogStep{")
	s = append(s, "StepId: "+fmt.Sprintf("%#v", this.StepId)+",\n")
	s = append(s, "Handle: "+fmt.Sprintf("%#v", this.Handle)+",\n")
	s = append(s, "}")
	return strings.Join(s, "")
}
func (this *MemoryLogTensorAllocation) GoString() string {
	if this == nil {
		return "nil"
	}
	s := make([]string, 0, 7)
	s = append(s, "&framework.MemoryLogTensorAllocation{")
	s = append(s, "StepId: "+fmt.Sprintf("%#v", this.StepId)+",\n")
	s = append(s, "KernelName: "+fmt.Sprintf("%#v", this.KernelName)+",\n")
	if this.Tensor != nil {
		s = append(s, "Tensor: "+fmt.Sprintf("%#v", this.Tensor)+",\n")
	}
	s = append(s, "}")
	return strings.Join(s, "")
}
func (this *MemoryLogTensorDeallocation) GoString() string {
	if this == nil {
		return "nil"
	}
	s := make([]string, 0, 6)
	s = append(s, "&framework.MemoryLogTensorDeallocation{")
	s = append(s, "AllocationId: "+fmt.Sprintf("%#v", this.AllocationId)+",\n")
	s = append(s, "AllocatorName: "+fmt.Sprintf("%#v", this.AllocatorName)+",\n")
	s = append(s, "}")
	return strings.Join(s, "")
}
func (this *MemoryLogTensorOutput) GoString() string {
	if this == nil {
		return "nil"
	}
	s := make([]string, 0, 8)
	s = append(s, "&framework.MemoryLogTensorOutput{")
	s = append(s, "StepId: "+fmt.Sprintf("%#v", this.StepId)+",\n")
	s = append(s, "KernelName: "+fmt.Sprintf("%#v", this.KernelName)+",\n")
	s = append(s, "Index: "+fmt.Sprintf("%#v", this.Index)+",\n")
	if this.Tensor != nil {
		s = append(s, "Tensor: "+fmt.Sprintf("%#v", this.Tensor)+",\n")
	}
	s = append(s, "}")
	return strings.Join(s, "")
}
func (this *MemoryLogRawAllocation) GoString() string {
	if this == nil {
		return "nil"
	}
	s := make([]string, 0, 10)
	s = append(s, "&framework.MemoryLogRawAllocation{")
	s = append(s, "StepId: "+fmt.Sprintf("%#v", this.StepId)+",\n")
	s = append(s, "Operation: "+fmt.Sprintf("%#v", this.Operation)+",\n")
	s = append(s, "NumBytes: "+fmt.Sprintf("%#v", this.NumBytes)+",\n")
	s = append(s, "Ptr: "+fmt.Sprintf("%#v", this.Ptr)+",\n")
	s = append(s, "AllocationId: "+fmt.Sprintf("%#v", this.AllocationId)+",\n")
	s = append(s, "AllocatorName: "+fmt.Sprintf("%#v", this.AllocatorName)+",\n")
	s = append(s, "}")
	return strings.Join(s, "")
}
func (this *MemoryLogRawDeallocation) GoString() string {
	if this == nil {
		return "nil"
	}
	s := make([]string, 0, 9)
	s = append(s, "&framework.MemoryLogRawDeallocation{")
	s = append(s, "StepId: "+fmt.Sprintf("%#v", this.StepId)+",\n")
	s = append(s, "Operation: "+fmt.Sprintf("%#v", this.Operation)+",\n")
	s = append(s, "AllocationId: "+fmt.Sprintf("%#v", this.AllocationId)+",\n")
	s = append(s, "AllocatorName: "+fmt.Sprintf("%#v", this.AllocatorName)+",\n")
	s = append(s, "Deferred: "+fmt.Sprintf("%#v", this.Deferred)+",\n")
	s = append(s, "}")
	return strings.Join(s, "")
}
func valueToGoStringLogMemory(v interface{}, typ string) string {
	rv := reflect.ValueOf(v)
	if rv.IsNil() {
		return "nil"
	}
	pv := reflect.Indirect(rv).Interface()
	return fmt.Sprintf("func(v %v) *%v { return &v } ( %#v )", typ, typ, pv)
}
func (m *MemoryLogStep) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalTo(dAtA)
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *MemoryLogStep) MarshalTo(dAtA []byte) (int, error) {
	var i int
	_ = i
	var l int
	_ = l
	if m.StepId != 0 {
		dAtA[i] = 0x8
		i++
		i = encodeVarintLogMemory(dAtA, i, uint64(m.StepId))
	}
	if len(m.Handle) > 0 {
		dAtA[i] = 0x12
		i++
		i = encodeVarintLogMemory(dAtA, i, uint64(len(m.Handle)))
		i += copy(dAtA[i:], m.Handle)
	}
	return i, nil
}

func (m *MemoryLogTensorAllocation) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalTo(dAtA)
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *MemoryLogTensorAllocation) MarshalTo(dAtA []byte) (int, error) {
	var i int
	_ = i
	var l int
	_ = l
	if m.StepId != 0 {
		dAtA[i] = 0x8
		i++
		i = encodeVarintLogMemory(dAtA, i, uint64(m.StepId))
	}
	if len(m.KernelName) > 0 {
		dAtA[i] = 0x12
		i++
		i = encodeVarintLogMemory(dAtA, i, uint64(len(m.KernelName)))
		i += copy(dAtA[i:], m.KernelName)
	}
	if m.Tensor != nil {
		dAtA[i] = 0x1a
		i++
		i = encodeVarintLogMemory(dAtA, i, uint64(m.Tensor.Size()))
		n1, err := m.Tensor.MarshalTo(dAtA[i:])
		if err != nil {
			return 0, err
		}
		i += n1
	}
	return i, nil
}

func (m *MemoryLogTensorDeallocation) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalTo(dAtA)
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *MemoryLogTensorDeallocation) MarshalTo(dAtA []byte) (int, error) {
	var i int
	_ = i
	var l int
	_ = l
	if m.AllocationId != 0 {
		dAtA[i] = 0x8
		i++
		i = encodeVarintLogMemory(dAtA, i, uint64(m.AllocationId))
	}
	if len(m.AllocatorName) > 0 {
		dAtA[i] = 0x12
		i++
		i = encodeVarintLogMemory(dAtA, i, uint64(len(m.AllocatorName)))
		i += copy(dAtA[i:], m.AllocatorName)
	}
	return i, nil
}

func (m *MemoryLogTensorOutput) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalTo(dAtA)
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *MemoryLogTensorOutput) MarshalTo(dAtA []byte) (int, error) {
	var i int
	_ = i
	var l int
	_ = l
	if m.StepId != 0 {
		dAtA[i] = 0x8
		i++
		i = encodeVarintLogMemory(dAtA, i, uint64(m.StepId))
	}
	if len(m.KernelName) > 0 {
		dAtA[i] = 0x12
		i++
		i = encodeVarintLogMemory(dAtA, i, uint64(len(m.KernelName)))
		i += copy(dAtA[i:], m.KernelName)
	}
	if m.Index != 0 {
		dAtA[i] = 0x18
		i++
		i = encodeVarintLogMemory(dAtA, i, uint64(m.Index))
	}
	if m.Tensor != nil {
		dAtA[i] = 0x22
		i++
		i = encodeVarintLogMemory(dAtA, i, uint64(m.Tensor.Size()))
		n2, err := m.Tensor.MarshalTo(dAtA[i:])
		if err != nil {
			return 0, err
		}
		i += n2
	}
	return i, nil
}

func (m *MemoryLogRawAllocation) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalTo(dAtA)
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *MemoryLogRawAllocation) MarshalTo(dAtA []byte) (int, error) {
	var i int
	_ = i
	var l int
	_ = l
	if m.StepId != 0 {
		dAtA[i] = 0x8
		i++
		i = encodeVarintLogMemory(dAtA, i, uint64(m.StepId))
	}
	if len(m.Operation) > 0 {
		dAtA[i] = 0x12
		i++
		i = encodeVarintLogMemory(dAtA, i, uint64(len(m.Operation)))
		i += copy(dAtA[i:], m.Operation)
	}
	if m.NumBytes != 0 {
		dAtA[i] = 0x18
		i++
		i = encodeVarintLogMemory(dAtA, i, uint64(m.NumBytes))
	}
	if m.Ptr != 0 {
		dAtA[i] = 0x20
		i++
		i = encodeVarintLogMemory(dAtA, i, uint64(m.Ptr))
	}
	if m.AllocationId != 0 {
		dAtA[i] = 0x28
		i++
		i = encodeVarintLogMemory(dAtA, i, uint64(m.AllocationId))
	}
	if len(m.AllocatorName) > 0 {
		dAtA[i] = 0x32
		i++
		i = encodeVarintLogMemory(dAtA, i, uint64(len(m.AllocatorName)))
		i += copy(dAtA[i:], m.AllocatorName)
	}
	return i, nil
}

func (m *MemoryLogRawDeallocation) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalTo(dAtA)
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *MemoryLogRawDeallocation) MarshalTo(dAtA []byte) (int, error) {
	var i int
	_ = i
	var l int
	_ = l
	if m.StepId != 0 {
		dAtA[i] = 0x8
		i++
		i = encodeVarintLogMemory(dAtA, i, uint64(m.StepId))
	}
	if len(m.Operation) > 0 {
		dAtA[i] = 0x12
		i++
		i = encodeVarintLogMemory(dAtA, i, uint64(len(m.Operation)))
		i += copy(dAtA[i:], m.Operation)
	}
	if m.AllocationId != 0 {
		dAtA[i] = 0x18
		i++
		i = encodeVarintLogMemory(dAtA, i, uint64(m.AllocationId))
	}
	if len(m.AllocatorName) > 0 {
		dAtA[i] = 0x22
		i++
		i = encodeVarintLogMemory(dAtA, i, uint64(len(m.AllocatorName)))
		i += copy(dAtA[i:], m.AllocatorName)
	}
	if m.Deferred {
		dAtA[i] = 0x28
		i++
		if m.Deferred {
			dAtA[i] = 1
		} else {
			dAtA[i] = 0
		}
		i++
	}
	return i, nil
}

func encodeVarintLogMemory(dAtA []byte, offset int, v uint64) int {
	for v >= 1<<7 {
		dAtA[offset] = uint8(v&0x7f | 0x80)
		v >>= 7
		offset++
	}
	dAtA[offset] = uint8(v)
	return offset + 1
}
func NewPopulatedMemoryLogStep(r randyLogMemory, easy bool) *MemoryLogStep {
	this := &MemoryLogStep{}
	this.StepId = int64(r.Int63())
	if r.Intn(2) == 0 {
		this.StepId *= -1
	}
	this.Handle = string(randStringLogMemory(r))
	if !easy && r.Intn(10) != 0 {
	}
	return this
}

func NewPopulatedMemoryLogTensorAllocation(r randyLogMemory, easy bool) *MemoryLogTensorAllocation {
	this := &MemoryLogTensorAllocation{}
	this.StepId = int64(r.Int63())
	if r.Intn(2) == 0 {
		this.StepId *= -1
	}
	this.KernelName = string(randStringLogMemory(r))
	if r.Intn(10) != 0 {
		this.Tensor = NewPopulatedTensorDescription(r, easy)
	}
	if !easy && r.Intn(10) != 0 {
	}
	return this
}

func NewPopulatedMemoryLogTensorDeallocation(r randyLogMemory, easy bool) *MemoryLogTensorDeallocation {
	this := &MemoryLogTensorDeallocation{}
	this.AllocationId = int64(r.Int63())
	if r.Intn(2) == 0 {
		this.AllocationId *= -1
	}
	this.AllocatorName = string(randStringLogMemory(r))
	if !easy && r.Intn(10) != 0 {
	}
	return this
}

func NewPopulatedMemoryLogTensorOutput(r randyLogMemory, easy bool) *MemoryLogTensorOutput {
	this := &MemoryLogTensorOutput{}
	this.StepId = int64(r.Int63())
	if r.Intn(2) == 0 {
		this.StepId *= -1
	}
	this.KernelName = string(randStringLogMemory(r))
	this.Index = int32(r.Int31())
	if r.Intn(2) == 0 {
		this.Index *= -1
	}
	if r.Intn(10) != 0 {
		this.Tensor = NewPopulatedTensorDescription(r, easy)
	}
	if !easy && r.Intn(10) != 0 {
	}
	return this
}

func NewPopulatedMemoryLogRawAllocation(r randyLogMemory, easy bool) *MemoryLogRawAllocation {
	this := &MemoryLogRawAllocation{}
	this.StepId = int64(r.Int63())
	if r.Intn(2) == 0 {
		this.StepId *= -1
	}
	this.Operation = string(randStringLogMemory(r))
	this.NumBytes = int64(r.Int63())
	if r.Intn(2) == 0 {
		this.NumBytes *= -1
	}
	this.Ptr = uint64(uint64(r.Uint32()))
	this.AllocationId = int64(r.Int63())
	if r.Intn(2) == 0 {
		this.AllocationId *= -1
	}
	this.AllocatorName = string(randStringLogMemory(r))
	if !easy && r.Intn(10) != 0 {
	}
	return this
}

func NewPopulatedMemoryLogRawDeallocation(r randyLogMemory, easy bool) *MemoryLogRawDeallocation {
	this := &MemoryLogRawDeallocation{}
	this.StepId = int64(r.Int63())
	if r.Intn(2) == 0 {
		this.StepId *= -1
	}
	this.Operation = string(randStringLogMemory(r))
	this.AllocationId = int64(r.Int63())
	if r.Intn(2) == 0 {
		this.AllocationId *= -1
	}
	this.AllocatorName = string(randStringLogMemory(r))
	this.Deferred = bool(bool(r.Intn(2) == 0))
	if !easy && r.Intn(10) != 0 {
	}
	return this
}

type randyLogMemory interface {
	Float32() float32
	Float64() float64
	Int63() int64
	Int31() int32
	Uint32() uint32
	Intn(n int) int
}

func randUTF8RuneLogMemory(r randyLogMemory) rune {
	ru := r.Intn(62)
	if ru < 10 {
		return rune(ru + 48)
	} else if ru < 36 {
		return rune(ru + 55)
	}
	return rune(ru + 61)
}
func randStringLogMemory(r randyLogMemory) string {
	v1 := r.Intn(100)
	tmps := make([]rune, v1)
	for i := 0; i < v1; i++ {
		tmps[i] = randUTF8RuneLogMemory(r)
	}
	return string(tmps)
}
func randUnrecognizedLogMemory(r randyLogMemory, maxFieldNumber int) (dAtA []byte) {
	l := r.Intn(5)
	for i := 0; i < l; i++ {
		wire := r.Intn(4)
		if wire == 3 {
			wire = 5
		}
		fieldNumber := maxFieldNumber + r.Intn(100)
		dAtA = randFieldLogMemory(dAtA, r, fieldNumber, wire)
	}
	return dAtA
}
func randFieldLogMemory(dAtA []byte, r randyLogMemory, fieldNumber int, wire int) []byte {
	key := uint32(fieldNumber)<<3 | uint32(wire)
	switch wire {
	case 0:
		dAtA = encodeVarintPopulateLogMemory(dAtA, uint64(key))
		v2 := r.Int63()
		if r.Intn(2) == 0 {
			v2 *= -1
		}
		dAtA = encodeVarintPopulateLogMemory(dAtA, uint64(v2))
	case 1:
		dAtA = encodeVarintPopulateLogMemory(dAtA, uint64(key))
		dAtA = append(dAtA, byte(r.Intn(256)), byte(r.Intn(256)), byte(r.Intn(256)), byte(r.Intn(256)), byte(r.Intn(256)), byte(r.Intn(256)), byte(r.Intn(256)), byte(r.Intn(256)))
	case 2:
		dAtA = encodeVarintPopulateLogMemory(dAtA, uint64(key))
		ll := r.Intn(100)
		dAtA = encodeVarintPopulateLogMemory(dAtA, uint64(ll))
		for j := 0; j < ll; j++ {
			dAtA = append(dAtA, byte(r.Intn(256)))
		}
	default:
		dAtA = encodeVarintPopulateLogMemory(dAtA, uint64(key))
		dAtA = append(dAtA, byte(r.Intn(256)), byte(r.Intn(256)), byte(r.Intn(256)), byte(r.Intn(256)))
	}
	return dAtA
}
func encodeVarintPopulateLogMemory(dAtA []byte, v uint64) []byte {
	for v >= 1<<7 {
		dAtA = append(dAtA, uint8(uint64(v)&0x7f|0x80))
		v >>= 7
	}
	dAtA = append(dAtA, uint8(v))
	return dAtA
}
func (m *MemoryLogStep) Size() (n int) {
	var l int
	_ = l
	if m.StepId != 0 {
		n += 1 + sovLogMemory(uint64(m.StepId))
	}
	l = len(m.Handle)
	if l > 0 {
		n += 1 + l + sovLogMemory(uint64(l))
	}
	return n
}

func (m *MemoryLogTensorAllocation) Size() (n int) {
	var l int
	_ = l
	if m.StepId != 0 {
		n += 1 + sovLogMemory(uint64(m.StepId))
	}
	l = len(m.KernelName)
	if l > 0 {
		n += 1 + l + sovLogMemory(uint64(l))
	}
	if m.Tensor != nil {
		l = m.Tensor.Size()
		n += 1 + l + sovLogMemory(uint64(l))
	}
	return n
}

func (m *MemoryLogTensorDeallocation) Size() (n int) {
	var l int
	_ = l
	if m.AllocationId != 0 {
		n += 1 + sovLogMemory(uint64(m.AllocationId))
	}
	l = len(m.AllocatorName)
	if l > 0 {
		n += 1 + l + sovLogMemory(uint64(l))
	}
	return n
}

func (m *MemoryLogTensorOutput) Size() (n int) {
	var l int
	_ = l
	if m.StepId != 0 {
		n += 1 + sovLogMemory(uint64(m.StepId))
	}
	l = len(m.KernelName)
	if l > 0 {
		n += 1 + l + sovLogMemory(uint64(l))
	}
	if m.Index != 0 {
		n += 1 + sovLogMemory(uint64(m.Index))
	}
	if m.Tensor != nil {
		l = m.Tensor.Size()
		n += 1 + l + sovLogMemory(uint64(l))
	}
	return n
}

func (m *MemoryLogRawAllocation) Size() (n int) {
	var l int
	_ = l
	if m.StepId != 0 {
		n += 1 + sovLogMemory(uint64(m.StepId))
	}
	l = len(m.Operation)
	if l > 0 {
		n += 1 + l + sovLogMemory(uint64(l))
	}
	if m.NumBytes != 0 {
		n += 1 + sovLogMemory(uint64(m.NumBytes))
	}
	if m.Ptr != 0 {
		n += 1 + sovLogMemory(uint64(m.Ptr))
	}
	if m.AllocationId != 0 {
		n += 1 + sovLogMemory(uint64(m.AllocationId))
	}
	l = len(m.AllocatorName)
	if l > 0 {
		n += 1 + l + sovLogMemory(uint64(l))
	}
	return n
}

func (m *MemoryLogRawDeallocation) Size() (n int) {
	var l int
	_ = l
	if m.StepId != 0 {
		n += 1 + sovLogMemory(uint64(m.StepId))
	}
	l = len(m.Operation)
	if l > 0 {
		n += 1 + l + sovLogMemory(uint64(l))
	}
	if m.AllocationId != 0 {
		n += 1 + sovLogMemory(uint64(m.AllocationId))
	}
	l = len(m.AllocatorName)
	if l > 0 {
		n += 1 + l + sovLogMemory(uint64(l))
	}
	if m.Deferred {
		n += 2
	}
	return n
}

func sovLogMemory(x uint64) (n int) {
	for {
		n++
		x >>= 7
		if x == 0 {
			break
		}
	}
	return n
}
func sozLogMemory(x uint64) (n int) {
	return sovLogMemory(uint64((x << 1) ^ uint64((int64(x) >> 63))))
}
func (this *MemoryLogStep) String() string {
	if this == nil {
		return "nil"
	}
	s := strings.Join([]string{`&MemoryLogStep{`,
		`StepId:` + fmt.Sprintf("%v", this.StepId) + `,`,
		`Handle:` + fmt.Sprintf("%v", this.Handle) + `,`,
		`}`,
	}, "")
	return s
}
func (this *MemoryLogTensorAllocation) String() string {
	if this == nil {
		return "nil"
	}
	s := strings.Join([]string{`&MemoryLogTensorAllocation{`,
		`StepId:` + fmt.Sprintf("%v", this.StepId) + `,`,
		`KernelName:` + fmt.Sprintf("%v", this.KernelName) + `,`,
		`Tensor:` + strings.Replace(fmt.Sprintf("%v", this.Tensor), "TensorDescription", "TensorDescription", 1) + `,`,
		`}`,
	}, "")
	return s
}
func (this *MemoryLogTensorDeallocation) String() string {
	if this == nil {
		return "nil"
	}
	s := strings.Join([]string{`&MemoryLogTensorDeallocation{`,
		`AllocationId:` + fmt.Sprintf("%v", this.AllocationId) + `,`,
		`AllocatorName:` + fmt.Sprintf("%v", this.AllocatorName) + `,`,
		`}`,
	}, "")
	return s
}
func (this *MemoryLogTensorOutput) String() string {
	if this == nil {
		return "nil"
	}
	s := strings.Join([]string{`&MemoryLogTensorOutput{`,
		`StepId:` + fmt.Sprintf("%v", this.StepId) + `,`,
		`KernelName:` + fmt.Sprintf("%v", this.KernelName) + `,`,
		`Index:` + fmt.Sprintf("%v", this.Index) + `,`,
		`Tensor:` + strings.Replace(fmt.Sprintf("%v", this.Tensor), "TensorDescription", "TensorDescription", 1) + `,`,
		`}`,
	}, "")
	return s
}
func (this *MemoryLogRawAllocation) String() string {
	if this == nil {
		return "nil"
	}
	s := strings.Join([]string{`&MemoryLogRawAllocation{`,
		`StepId:` + fmt.Sprintf("%v", this.StepId) + `,`,
		`Operation:` + fmt.Sprintf("%v", this.Operation) + `,`,
		`NumBytes:` + fmt.Sprintf("%v", this.NumBytes) + `,`,
		`Ptr:` + fmt.Sprintf("%v", this.Ptr) + `,`,
		`AllocationId:` + fmt.Sprintf("%v", this.AllocationId) + `,`,
		`AllocatorName:` + fmt.Sprintf("%v", this.AllocatorName) + `,`,
		`}`,
	}, "")
	return s
}
func (this *MemoryLogRawDeallocation) String() string {
	if this == nil {
		return "nil"
	}
	s := strings.Join([]string{`&MemoryLogRawDeallocation{`,
		`StepId:` + fmt.Sprintf("%v", this.StepId) + `,`,
		`Operation:` + fmt.Sprintf("%v", this.Operation) + `,`,
		`AllocationId:` + fmt.Sprintf("%v", this.AllocationId) + `,`,
		`AllocatorName:` + fmt.Sprintf("%v", this.AllocatorName) + `,`,
		`Deferred:` + fmt.Sprintf("%v", this.Deferred) + `,`,
		`}`,
	}, "")
	return s
}
func valueToStringLogMemory(v interface{}) string {
	rv := reflect.ValueOf(v)
	if rv.IsNil() {
		return "nil"
	}
	pv := reflect.Indirect(rv).Interface()
	return fmt.Sprintf("*%v", pv)
}
func (m *MemoryLogStep) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowLogMemory
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= (uint64(b) & 0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: MemoryLogStep: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: MemoryLogStep: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field StepId", wireType)
			}
			m.StepId = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowLogMemory
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.StepId |= (int64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 2:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field Handle", wireType)
			}
			var stringLen uint64
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowLogMemory
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				stringLen |= (uint64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			intStringLen := int(stringLen)
			if intStringLen < 0 {
				return ErrInvalidLengthLogMemory
			}
			postIndex := iNdEx + intStringLen
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.Handle = string(dAtA[iNdEx:postIndex])
			iNdEx = postIndex
		default:
			iNdEx = preIndex
			skippy, err := skipLogMemory(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthLogMemory
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
func (m *MemoryLogTensorAllocation) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowLogMemory
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= (uint64(b) & 0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: MemoryLogTensorAllocation: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: MemoryLogTensorAllocation: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field StepId", wireType)
			}
			m.StepId = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowLogMemory
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.StepId |= (int64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 2:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field KernelName", wireType)
			}
			var stringLen uint64
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowLogMemory
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				stringLen |= (uint64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			intStringLen := int(stringLen)
			if intStringLen < 0 {
				return ErrInvalidLengthLogMemory
			}
			postIndex := iNdEx + intStringLen
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.KernelName = string(dAtA[iNdEx:postIndex])
			iNdEx = postIndex
		case 3:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field Tensor", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowLogMemory
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= (int(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthLogMemory
			}
			postIndex := iNdEx + msglen
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			if m.Tensor == nil {
				m.Tensor = &TensorDescription{}
			}
			if err := m.Tensor.Unmarshal(dAtA[iNdEx:postIndex]); err != nil {
				return err
			}
			iNdEx = postIndex
		default:
			iNdEx = preIndex
			skippy, err := skipLogMemory(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthLogMemory
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
func (m *MemoryLogTensorDeallocation) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowLogMemory
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= (uint64(b) & 0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: MemoryLogTensorDeallocation: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: MemoryLogTensorDeallocation: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field AllocationId", wireType)
			}
			m.AllocationId = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowLogMemory
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.AllocationId |= (int64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 2:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field AllocatorName", wireType)
			}
			var stringLen uint64
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowLogMemory
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				stringLen |= (uint64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			intStringLen := int(stringLen)
			if intStringLen < 0 {
				return ErrInvalidLengthLogMemory
			}
			postIndex := iNdEx + intStringLen
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.AllocatorName = string(dAtA[iNdEx:postIndex])
			iNdEx = postIndex
		default:
			iNdEx = preIndex
			skippy, err := skipLogMemory(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthLogMemory
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
func (m *MemoryLogTensorOutput) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowLogMemory
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= (uint64(b) & 0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: MemoryLogTensorOutput: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: MemoryLogTensorOutput: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field StepId", wireType)
			}
			m.StepId = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowLogMemory
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.StepId |= (int64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 2:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field KernelName", wireType)
			}
			var stringLen uint64
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowLogMemory
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				stringLen |= (uint64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			intStringLen := int(stringLen)
			if intStringLen < 0 {
				return ErrInvalidLengthLogMemory
			}
			postIndex := iNdEx + intStringLen
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.KernelName = string(dAtA[iNdEx:postIndex])
			iNdEx = postIndex
		case 3:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field Index", wireType)
			}
			m.Index = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowLogMemory
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.Index |= (int32(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 4:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field Tensor", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowLogMemory
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= (int(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthLogMemory
			}
			postIndex := iNdEx + msglen
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			if m.Tensor == nil {
				m.Tensor = &TensorDescription{}
			}
			if err := m.Tensor.Unmarshal(dAtA[iNdEx:postIndex]); err != nil {
				return err
			}
			iNdEx = postIndex
		default:
			iNdEx = preIndex
			skippy, err := skipLogMemory(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthLogMemory
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
func (m *MemoryLogRawAllocation) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowLogMemory
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= (uint64(b) & 0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: MemoryLogRawAllocation: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: MemoryLogRawAllocation: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field StepId", wireType)
			}
			m.StepId = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowLogMemory
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.StepId |= (int64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 2:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field Operation", wireType)
			}
			var stringLen uint64
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowLogMemory
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				stringLen |= (uint64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			intStringLen := int(stringLen)
			if intStringLen < 0 {
				return ErrInvalidLengthLogMemory
			}
			postIndex := iNdEx + intStringLen
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.Operation = string(dAtA[iNdEx:postIndex])
			iNdEx = postIndex
		case 3:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field NumBytes", wireType)
			}
			m.NumBytes = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowLogMemory
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.NumBytes |= (int64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 4:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field Ptr", wireType)
			}
			m.Ptr = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowLogMemory
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.Ptr |= (uint64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 5:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field AllocationId", wireType)
			}
			m.AllocationId = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowLogMemory
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.AllocationId |= (int64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 6:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field AllocatorName", wireType)
			}
			var stringLen uint64
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowLogMemory
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				stringLen |= (uint64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			intStringLen := int(stringLen)
			if intStringLen < 0 {
				return ErrInvalidLengthLogMemory
			}
			postIndex := iNdEx + intStringLen
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.AllocatorName = string(dAtA[iNdEx:postIndex])
			iNdEx = postIndex
		default:
			iNdEx = preIndex
			skippy, err := skipLogMemory(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthLogMemory
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
func (m *MemoryLogRawDeallocation) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowLogMemory
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= (uint64(b) & 0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: MemoryLogRawDeallocation: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: MemoryLogRawDeallocation: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field StepId", wireType)
			}
			m.StepId = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowLogMemory
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.StepId |= (int64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 2:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field Operation", wireType)
			}
			var stringLen uint64
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowLogMemory
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				stringLen |= (uint64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			intStringLen := int(stringLen)
			if intStringLen < 0 {
				return ErrInvalidLengthLogMemory
			}
			postIndex := iNdEx + intStringLen
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.Operation = string(dAtA[iNdEx:postIndex])
			iNdEx = postIndex
		case 3:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field AllocationId", wireType)
			}
			m.AllocationId = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowLogMemory
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.AllocationId |= (int64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 4:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field AllocatorName", wireType)
			}
			var stringLen uint64
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowLogMemory
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				stringLen |= (uint64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			intStringLen := int(stringLen)
			if intStringLen < 0 {
				return ErrInvalidLengthLogMemory
			}
			postIndex := iNdEx + intStringLen
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.AllocatorName = string(dAtA[iNdEx:postIndex])
			iNdEx = postIndex
		case 5:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field Deferred", wireType)
			}
			var v int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowLogMemory
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				v |= (int(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			m.Deferred = bool(v != 0)
		default:
			iNdEx = preIndex
			skippy, err := skipLogMemory(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthLogMemory
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
func skipLogMemory(dAtA []byte) (n int, err error) {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return 0, ErrIntOverflowLogMemory
			}
			if iNdEx >= l {
				return 0, io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= (uint64(b) & 0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		wireType := int(wire & 0x7)
		switch wireType {
		case 0:
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return 0, ErrIntOverflowLogMemory
				}
				if iNdEx >= l {
					return 0, io.ErrUnexpectedEOF
				}
				iNdEx++
				if dAtA[iNdEx-1] < 0x80 {
					break
				}
			}
			return iNdEx, nil
		case 1:
			iNdEx += 8
			return iNdEx, nil
		case 2:
			var length int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return 0, ErrIntOverflowLogMemory
				}
				if iNdEx >= l {
					return 0, io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				length |= (int(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			iNdEx += length
			if length < 0 {
				return 0, ErrInvalidLengthLogMemory
			}
			return iNdEx, nil
		case 3:
			for {
				var innerWire uint64
				var start int = iNdEx
				for shift := uint(0); ; shift += 7 {
					if shift >= 64 {
						return 0, ErrIntOverflowLogMemory
					}
					if iNdEx >= l {
						return 0, io.ErrUnexpectedEOF
					}
					b := dAtA[iNdEx]
					iNdEx++
					innerWire |= (uint64(b) & 0x7F) << shift
					if b < 0x80 {
						break
					}
				}
				innerWireType := int(innerWire & 0x7)
				if innerWireType == 4 {
					break
				}
				next, err := skipLogMemory(dAtA[start:])
				if err != nil {
					return 0, err
				}
				iNdEx = start + next
			}
			return iNdEx, nil
		case 4:
			return iNdEx, nil
		case 5:
			iNdEx += 4
			return iNdEx, nil
		default:
			return 0, fmt.Errorf("proto: illegal wireType %d", wireType)
		}
	}
	panic("unreachable")
}

var (
	ErrInvalidLengthLogMemory = fmt.Errorf("proto: negative length found during unmarshaling")
	ErrIntOverflowLogMemory   = fmt.Errorf("proto: integer overflow")
)

func init() { proto.RegisterFile("log_memory.proto", fileDescriptor_log_memory_169de76e43ea0cf0) }

var fileDescriptor_log_memory_169de76e43ea0cf0 = []byte{
	// 475 bytes of a gzipped FileDescriptorProto
	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0xff, 0xa4, 0x93, 0xc1, 0x6e, 0xd3, 0x40,
	0x10, 0x86, 0x3b, 0x38, 0x31, 0xc9, 0x94, 0x40, 0x65, 0x41, 0x31, 0x69, 0xb5, 0x44, 0x46, 0x48,
	0x39, 0xa5, 0x12, 0xf0, 0x00, 0x10, 0xf5, 0x52, 0xa9, 0x40, 0x65, 0x38, 0x22, 0x59, 0x4e, 0x3d,
	0x31, 0x56, 0x6d, 0xaf, 0xb5, 0x5e, 0x2b, 0xf4, 0xc6, 0x9d, 0x0b, 0x2f, 0xc0, 0x9d, 0x47, 0x80,
	0x37, 0x40, 0x9c, 0x38, 0x72, 0x6c, 0xcc, 0x0b, 0x70, 0xec, 0x11, 0x79, 0x37, 0xd8, 0xa1, 0x2d,
	0x52, 0xa0, 0x37, 0xff, 0xe3, 0xd1, 0xcc, 0xf7, 0xff, 0xa3, 0xc5, 0x8d, 0x98, 0x87, 0x5e, 0x42,
	0x09, 0x17, 0xc7, 0xa3, 0x4c, 0x70, 0xc9, 0xad, 0xee, 0x54, 0xf8, 0x09, 0xcd, 0xb8, 0x38, 0xea,
	0x3b, 0x21, 0x0f, 0xf9, 0x8e, 0x2a, 0x4f, 0x8a, 0xe9, 0x4e, 0xa5, 0x94, 0x50, 0x5f, 0xba, 0xbd,
	0x6f, 0x4b, 0x4a, 0x73, 0x2e, 0xbc, 0x80, 0xf2, 0x43, 0x11, 0x65, 0x32, 0xe2, 0xa9, 0xfe, 0xe3,
	0x3c, 0xc6, 0xde, 0x53, 0x35, 0x78, 0x9f, 0x87, 0x2f, 0x24, 0x65, 0xd6, 0x6d, 0xbc, 0x9a, 0x4b,
	0xca, 0xbc, 0x28, 0xb0, 0x61, 0x00, 0x43, 0xc3, 0x35, 0x2b, 0xb9, 0x17, 0x58, 0x9b, 0x68, 0xbe,
	0xf6, 0xd3, 0x20, 0x26, 0xfb, 0xca, 0x00, 0x86, 0x5d, 0x77, 0xa1, 0x9c, 0x77, 0x80, 0x77, 0xea,
	0x11, 0x2f, 0xd5, 0x9e, 0x27, 0x71, 0xcc, 0x0f, 0xfd, 0x6a, 0xcb, 0xdf, 0xc7, 0xdd, 0xc5, 0xf5,
	0x23, 0x12, 0x29, 0xc5, 0x5e, 0xea, 0x27, 0xbf, 0x67, 0xa2, 0x2e, 0x3d, 0xf3, 0x13, 0xb2, 0x1e,
	0xa1, 0xa9, 0xa9, 0x6d, 0x63, 0x00, 0xc3, 0xf5, 0x07, 0xdb, 0xa3, 0xda, 0xf3, 0x48, 0xaf, 0xd9,
	0x6d, 0xdc, 0xb8, 0x8b, 0x5e, 0x27, 0xc2, 0xad, 0x33, 0x30, 0xbb, 0xe4, 0x37, 0x38, 0xf7, 0xb0,
	0xd7, 0xa8, 0x06, 0xea, 0x5a, 0x53, 0xdc, 0x0b, 0xac, 0xfb, 0x78, 0x7d, 0xa1, 0xb9, 0x58, 0xa6,
	0xeb, 0xd5, 0xd5, 0x0a, 0xd0, 0xf9, 0x00, 0x78, 0xeb, 0xcc, 0xae, 0xe7, 0x85, 0xcc, 0x0a, 0x79,
	0x09, 0xd3, 0x37, 0xb1, 0x1d, 0xa5, 0x01, 0xbd, 0x51, 0x9e, 0xdb, 0xae, 0x16, 0x4b, 0x51, 0xb4,
	0xfe, 0x21, 0x8a, 0xaf, 0x80, 0x9b, 0x35, 0x9f, 0xeb, 0xcf, 0x56, 0xb9, 0xca, 0x36, 0x76, 0x79,
	0x46, 0x42, 0x75, 0x2d, 0xf0, 0x9a, 0x82, 0xb5, 0x85, 0xdd, 0xb4, 0x48, 0xbc, 0xc9, 0xb1, 0xa4,
	0x5c, 0x11, 0x1a, 0x6e, 0x27, 0x2d, 0x92, 0x71, 0xa5, 0xad, 0x0d, 0x34, 0x32, 0xa9, 0x09, 0x5b,
	0x6e, 0xf5, 0x79, 0x3e, 0xec, 0xf6, 0x4a, 0x61, 0x9b, 0x17, 0x85, 0xfd, 0x19, 0xd0, 0x5e, 0x36,
	0xf3, 0xc7, 0x55, 0xff, 0xd3, 0xce, 0x39, 0x3e, 0x63, 0x25, 0xbe, 0xd6, 0x05, 0x7c, 0x56, 0x1f,
	0x3b, 0x01, 0x4d, 0x49, 0x08, 0xd2, 0x36, 0x3b, 0x6e, 0xad, 0xc7, 0xaf, 0xbe, 0xcf, 0xd9, 0xda,
	0xc9, 0x9c, 0xc1, 0xcf, 0x39, 0x83, 0xd3, 0x39, 0x83, 0xb7, 0x25, 0x83, 0x8f, 0x25, 0x83, 0x4f,
	0x25, 0x83, 0x2f, 0x25, 0x83, 0x6f, 0x25, 0x83, 0x93, 0x92, 0xc1, 0xfb, 0x1f, 0x6c, 0x0d, 0x6d,
	0x2e, 0xc2, 0x91, 0x3e, 0xe1, 0x34, 0xe6, 0xb3, 0xe6, 0xd4, 0xe3, 0x1b, 0xfb, 0x3c, 0xd4, 0x19,
	0x1c, 0x54, 0x6f, 0x37, 0x3f, 0x80, 0x53, 0x80, 0x89, 0xa9, 0x1e, 0xf2, 0xc3, 0x5f, 0x01, 0x00,
	0x00, 0xff, 0xff, 0x14, 0x20, 0xad, 0x80, 0x25, 0x04, 0x00, 0x00,
}
